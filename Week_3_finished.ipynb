{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Consider the unconstrained optimisation problem\n",
        "\n",
        "\\begin{align}\n",
        "\\text{min  } f(x_1, x_2) = 4x_1^2 - 4x_1^4 + x_1^{2} +x_1x_2 - 4x_2^2 + 4x_2^4\n",
        "\\end{align}\n",
        "\n",
        "# Another problem\n",
        "\\begin{align}\n",
        "\\text{min  } f(x_1, x_2) = e^{x_1}(4x_1^2 + 2x_2^2 + 4x_1x_2 + 2x_2 + 1)\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pL9ds0fSfauN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import autograd.numpy as np \n",
        "import autograd.numpy.linalg as la\n",
        "from autograd import grad, jacobian"
      ],
      "metadata": {
        "id": "JcsbqvpOx5T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = lambda x: (4*x[0]**2) - (4*x[0]**4) + (x[0]**(2) + x[0]*x[1]) - (4*x[1]**2) + (4*x[1]**4)\n",
        "f([0, 0])"
      ],
      "metadata": {
        "id": "Q9_6K2MjGpE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = lambda x: np.exp(x[0]) * (4*x[0]**2 + 2*x[1]**2 + 4*x[0]*x[1] + 2*x[1] + 1)\n",
        "f1([0, 0])"
      ],
      "metadata": {
        "id": "gKxx5JkByVVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# visualise the objective function with Matplotlib\n",
        "A Colab tutorial for Matplotlib can be found here [Colab tutorial on Matplotlib](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.00-Introduction-To-Matplotlib.ipynb#scrollTo=AsOD563_e8Ge). We should use the Object-oriented interface as explained in this tutorial. More details can be found in the book [Python Data Science Handbook](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb)"
      ],
      "metadata": {
        "id": "ytqQghfMs2kJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "jSBPGZE8zYHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To draw contour graph for a two-variables function, please read [Contour **plots**](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.04-Density-and-Contour-Plots.ipynb)"
      ],
      "metadata": {
        "id": "ya3xL6kb7b9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.linspace(0, 1, 100)\n",
        "x2 = np.linspace(0, 1, 100)\n",
        "\n",
        "X1, X2 = np.meshgrid(x1, x2)\n",
        "Z = f((X1, X2))\n"
      ],
      "metadata": {
        "id": "4t1Knp4X7sn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z.shape"
      ],
      "metadata": {
        "id": "mlWBcJr_FEiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (14.7, 8.27))\n",
        "#fig.set_size_inches(14.7, 8.27)\n",
        "plt.contour(X1, X2, Z, 50,cmap='jet')\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "GyylvfP1AaAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The contour may be misleading if the region is too big. You can create a function to visualise a specified region of the contour. How many local minima can you find?"
      ],
      "metadata": {
        "id": "Yib1wPiSxJ4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VtQnIQB4xgfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# we can solve a non-linear programming problem using scipyâ€™s minimize function [scipy.optimize](https://docs.scipy.org/doc/scipy/tutorial/optimize.html).\n",
        "\n",
        " We use the Broyden-Fletcher-Goldfarb-Shanno algorithm (BFGS), and examples for other methods can be found here [minimize](https://docs.scipy.org/doc/scipy/tutorial/optimize.html#unconstrained-minimization-of-multivariate-scalar-functions-minimize)\n",
        "\n",
        " BFGS requires gradient, which can be automatically calculated by autograd (see notebook from last week)"
      ],
      "metadata": {
        "id": "EBNixBlTxjkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autograd import grad, jacobian\n",
        "from scipy.optimize import minimize, OptimizeResult\n"
      ],
      "metadata": {
        "id": "YSOMyhaZs-Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_f = grad(f)\n",
        "hessen_f = jacobian(grad_f)\n"
      ],
      "metadata": {
        "id": "Vxzgp5SO0UHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try initial point $x_0 = [0.5, 0.5]$"
      ],
      "metadata": {
        "id": "PlTSGv8_33bL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = np.array([0.5, 0.5]) #initial point\n",
        "print(\" gradient = \", grad_f(x0), \"\\n Hessian = \", hessen_f(x0))"
      ],
      "metadata": {
        "id": "SNugcqAT3f0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = minimize(f, x0, method='BFGS', jac = grad_f, \\\n",
        "               options={'disp': True})\n",
        "print(\"-\"*80)\n",
        "res"
      ],
      "metadata": {
        "id": "9lP27lnH0ieK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try different initial points and compare with the contour plot"
      ],
      "metadata": {
        "id": "ECWx22Xm4AOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Line search\n",
        "Steepest descent method has a line search step to find a better solution along the negative gradient direction. [Line search](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.line_search.html#scipy.optimize.line_search) is not trivial since convergence needs to be maintained while computational effort is kept reasonable.  "
      ],
      "metadata": {
        "id": "xI_sEml65e7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import line_search"
      ],
      "metadata": {
        "id": "tKDz44Tq8WAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = np.array([0.5, 0.5])\n",
        "res = line_search(f, grad_f, x0, -1.*grad_f(x0))\n",
        "res"
      ],
      "metadata": {
        "id": "ajAZmWt38cxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum step size is necessary since line_search may not converge. Maximum number of iteration is also worth trying"
      ],
      "metadata": {
        "id": "6UatOsFI9-r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = line_search(f, grad_f, x0, -1*grad_f(x0), amax = 0.1, maxiter = 3)\n",
        "res"
      ],
      "metadata": {
        "id": "UG5KUWKg-xNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may play with other parameters, but can avoid all these difficulties using fixed step size without line search"
      ],
      "metadata": {
        "id": "yjmHu4gy_w6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f0 = f(x0)\n",
        "print(\"f0 = \", f0, \"f1 = \", f(x0 - 1.e-1*grad_f(x0)))"
      ],
      "metadata": {
        "id": "CwzvHIXhAFtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of the Steepest Descent Method"
      ],
      "metadata": {
        "id": "9kZ2gUCfFWNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def steepest_descent(fun, x0, jac, ls = line_search, maxiter = 100, amax = 1000., tol = 1.e-8 ):\n",
        "  ''' Simple implementation of Steepest Descent for minimising unconstrained nonlinear function.\n",
        "\n",
        "  Parameters: \n",
        "    fun (callable) : the function to minimise\n",
        "\n",
        "    x0 (1D array) : initial guess\n",
        "\n",
        "    jac (callable) : the gradient function\n",
        "\n",
        "    ls (callable, line_search)\n",
        "\n",
        "    maxiter(int, 100) : maximum number of iterations\n",
        "\n",
        "    amax(float, 1000.) : max step size in line search\n",
        "\n",
        "    tol(float, 1.e-8) : used for stopping criteria\n",
        "\n",
        "  Return:\n",
        "    res (scipy.optimize.OptimizeResult): optimal solution and value\n",
        "\n",
        "  Note:\n",
        "    Follow the style of scipy.optimize;\n",
        "    scipy.optimize.line_search is used\n",
        "\n",
        "  '''\n",
        "  x_eps = tol # tolerence for convergence on delta x\n",
        "  f_eps = tol # tolerence for convergence on delta f\n",
        "  g_eps = tol # tolerence for convergence on norm of gradient\n",
        "  x_k = x0.copy()\n",
        "  f_k = fun(x_k)\n",
        "  nfev = 1\n",
        "  g_k = jac(x_k)\n",
        "  njev = 1\n",
        "  nit = 1\n",
        "  res = OptimizeResult()\n",
        "  n_g_k = la.norm(g_k)\n",
        "  if n_g_k < g_eps:\n",
        "    res.x = x_k\n",
        "    res.success = True\n",
        "    res.status = 0\n",
        "    res.message = \"norm of gradient is within tolerence\"\n",
        "    res.fun = f_k\n",
        "    res.nfev = nfev\n",
        "    res.njev = njev\n",
        "    res.nit = nit\n",
        "\n",
        "  while True:\n",
        "    alpha_k, fc_k, gc_k, f_k1, _,_ = ls(fun,jac,x_k,-g_k , amax=amax)\n",
        "    nfev += fc_k\n",
        "    njev += gc_k\n",
        "    if alpha_k == None or f_k1 == None: \n",
        "      res.x = x_k\n",
        "      res.success = False\n",
        "      res.status = 1\n",
        "      res.message = \"Line search fail: alpha or fun is None\"\n",
        "      res.fun = f_k\n",
        "      res.nfev = nfev\n",
        "      res.njev = njev\n",
        "      res.nit = nit\n",
        "      break\n",
        "\n",
        "    if abs(alpha_k*n_g_k) < x_eps:\n",
        "      res.x = x_k1\n",
        "      res.success = True\n",
        "      res.status = 0\n",
        "      res.message = \"change of x is within tolerence\"\n",
        "      res.fun = f_k1\n",
        "      res.nfev = nfev\n",
        "      res.njev = njev\n",
        "      res.nit = nit\n",
        "      break      \n",
        "    \n",
        "    x_k1 = x_k - alpha_k*g_k\n",
        "    \n",
        "    if abs(f_k - f_k1) < f_eps:\n",
        "      res.x = x_k1\n",
        "      res.success = True\n",
        "      res.status = 0\n",
        "      res.message = \"change of fun is within tolerence\"\n",
        "      res.fun = f_k1\n",
        "      res.nfev = nfev\n",
        "      res.njev = njev\n",
        "      res.nit = nit\n",
        "      break      \n",
        "    \n",
        "    g_k1 = jac(x_k1)\n",
        "    njev += 1\n",
        "    n_g_k = la.norm(g_k1)\n",
        "    if n_g_k < g_eps:\n",
        "      res.x = x_k1\n",
        "      res.success = True\n",
        "      res.status = 0\n",
        "      res.message = \"norm of gradient is within tolerence\"\n",
        "      res.fun = f_k1\n",
        "      res.nfev = nfev\n",
        "      res.njev = njev\n",
        "      res.nit = nit\n",
        "      break      \n",
        "\n",
        "    if nit > maxiter:\n",
        "      res.x = x_k1\n",
        "      res.success = False\n",
        "      res.status = 0\n",
        "      res.message = \"Max iter reached\"\n",
        "      res.fun = f_k1\n",
        "      res.nfev = nfev\n",
        "      res.njev = njev\n",
        "      res.nit = nit\n",
        "      break     \n",
        "\n",
        "    nit += 1\n",
        "    x_k = x_k1\n",
        "    f_k = f_k1\n",
        "  return res"
      ],
      "metadata": {
        "id": "yhJ3oeExYV49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Docstring makes documentation easy"
      ],
      "metadata": {
        "id": "uMMuPmcHznem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(steepest_descent)"
      ],
      "metadata": {
        "id": "uBrlPmU8Yp0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steepest_descent(f, x0, grad_f)"
      ],
      "metadata": {
        "id": "9ZbYI0DxrR3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = np.array([0.5, 0.5])\n",
        "steepest_descent(f, x0, grad_f)"
      ],
      "metadata": {
        "id": "NastMEbQsv10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = np.array([-0.071,  0.71]) # very close to a local optimum\n",
        "steepest_descent(f, x0, grad_f, amax = 0.1)"
      ],
      "metadata": {
        "id": "aFada5JwtL6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We can try different line search method"
      ],
      "metadata": {
        "id": "gQu4DHcCzaG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize.linesearch import line_search_wolfe1\n"
      ],
      "metadata": {
        "id": "v34U1F4txL1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = np.array([-0.071,  0.71]) # very close to a local optimum\n",
        "steepest_descent(f, x0, grad_f, ls = line_search_wolfe1, amax = 1.)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8JncXuUExSTx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Week_3_finished.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}