{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import autograd.numpy as np\n",
    "from autograd import grad, jacobian\n",
    "from scipy.optimize import line_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix definiteness\n",
    "\n",
    "- positive (semi-positive) definite : all eigenvalue positive (non-negative)\n",
    "- negative (semi-negative) definite : all eigenvalue negative (non-possible)\n",
    "- indefinite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def positive_definite(A):\n",
    "    \"\"\"\n",
    "    Check if the matrix A is positive definite.\n",
    "    \"\"\"\n",
    "    return np.all(la.eigvals(A) > 0)\n",
    "\n",
    "\n",
    "def negative_definite(A):\n",
    "    \"\"\"\n",
    "    Check if the matrix A is positive definite.\n",
    "    \"\"\"\n",
    "    return np.all(la.eigvals(A) < 0)\n",
    "\n",
    "\n",
    "def semi_positive_definite(A):\n",
    "    \"\"\"\n",
    "    Check if the matrix A is positive definite.\n",
    "    \"\"\"\n",
    "    return np.all(la.eigvals(A) >= 0)\n",
    "\n",
    "\n",
    "def semi_negative_definite(A):\n",
    "    \"\"\"\n",
    "    Check if the matrix A is positive definite.\n",
    "    \"\"\"\n",
    "    return np.all(la.eigvals(A) <= 0)\n",
    "\n",
    "\n",
    "A = np.array([[3, 1, 5], [1, 4, 2], [5, 2, 1]])\n",
    "\n",
    "\n",
    "positive_definite(A)  # Check if A is positive definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix symetry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def symetry(A: np.ndarray):\n",
    "    \"\"\"\n",
    "    Check if the matrix A is positive definite.\n",
    "    \"\"\"\n",
    "    if len(A.shape) != 2:\n",
    "        raise False\n",
    "    if A.shape[0] != A.shape[1]:\n",
    "        raise False\n",
    "    return np.allclose(A, A.T)\n",
    "\n",
    "\n",
    "A = np.array([[3, 1, 5], [1, 4, 2], [5, 2, 1]])\n",
    "symetry(A)  # Check if A is positive definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix inverse\n",
    "\n",
    "A square matrix is invertible if\n",
    "\n",
    "- determinant is not zero\n",
    "- SVD?\n",
    "- Eigenvalue?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-80.99999999999996)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-differentiation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " gradient =  [ 0. -3.] \n",
      " Hessian =  [[2. 1.]\n",
      " [1. 2.]] \n",
      " determinant =  2.9999999999999996\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return np.sum(x**2) + x[0] * x[1] - x[0] - 2 * x[1]\n",
    "\n",
    "\n",
    "grad_f = grad(f)\n",
    "hessen_f = jacobian(grad_f)\n",
    "x = np.array([1.0, -1.0])\n",
    "print(\n",
    "    \" gradient = \",\n",
    "    grad_f(x),\n",
    "    \"\\n Hessian = \",\n",
    "    hessen_f(x),\n",
    "    \"\\n determinant = \",\n",
    "    la.det(hessen_f(x)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steepest Descent Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(\n",
    "    fun, x0: np.ndarray, jac, ls=line_search, maxiter=100, amax=1000.0, tol=1.0e-8\n",
    "):\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    d_k = -jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation: \", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        alpha_k, _, _, _, _, success = ls(fun, jac, x_k, d_k, amax=amax)\n",
    "        if success is None:\n",
    "            print(\"Line search fail\")\n",
    "            break\n",
    "\n",
    "        print(\"alpha_k = \", alpha_k)\n",
    "        if abs(alpha_k * la.norm(jac(x_k))) < x_eps:\n",
    "            print(\"change of x is within tolerence\")\n",
    "            break\n",
    "\n",
    "        x_k1 = x_k + alpha_k * d_k\n",
    "        d_k1 = -jac(x_k1)\n",
    "\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fletcher-Reeves Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fletcher_reeves(\n",
    "    fun, x0: np.ndarray, jac, ls=line_search, maxiter=100, amax=1000.0, tol=1.0e-8\n",
    "):\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    d_k = -jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation: \", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        alpha_k, _, _, _, _, success = ls(fun, jac, x_k, d_k, amax=amax)\n",
    "        if success is None:\n",
    "            print(\"Line search fail\")\n",
    "            break\n",
    "\n",
    "        print(\"alpha_k = \", alpha_k)\n",
    "        if abs(alpha_k * la.norm(jac(x_k))) < x_eps:\n",
    "            print(\"change of x is within tolerence\")\n",
    "            break\n",
    "\n",
    "        x_k1 = x_k + alpha_k * d_k\n",
    "        d_k1 = -jac(x_k1) + (la.norm(jac(x_k1)) ** 2 / la.norm(jac(x_k)) ** 2) * d_k\n",
    "\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fletcher-Reeves Reset Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fletcher_reeves_reset(\n",
    "    fun,\n",
    "    x0: np.ndarray,\n",
    "    jac,\n",
    "    ls=line_search,\n",
    "    maxiter=100,\n",
    "    amax=1000.0,\n",
    "    tol=1.0e-8,\n",
    "    reset=100,\n",
    "):\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    d_k = -jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation:\", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        alpha_k, _, _, _, _, success = ls(fun, jac, x_k, d_k, amax=amax)\n",
    "        if success is None:\n",
    "            print(\"Line search fail\")\n",
    "            break\n",
    "\n",
    "        print(\"alpha_k = \", alpha_k)\n",
    "        if abs(alpha_k * la.norm(jac(x_k))) < x_eps:\n",
    "            print(\"change of x is within tolerence\")\n",
    "            break\n",
    "        x_k1 = x_k + alpha_k * d_k\n",
    "        d_k1 = None\n",
    "        if nit % reset == 0:\n",
    "            d_k1 = -jac(x_k1)\n",
    "        else:\n",
    "            d_k1 = -jac(x_k1) + (la.norm(jac(x_k1)) ** 2 / la.norm(jac(x_k)) ** 2) * d_k\n",
    "\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(\n",
    "    fun,\n",
    "    x0: np.ndarray,\n",
    "    jac,\n",
    "    ls=line_search,\n",
    "    maxiter=100,\n",
    "    amax=1000.0,\n",
    "    tol=1.0e-8,\n",
    "):\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    hessian = jacobian(jac)\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    d_k = -la.inv(hessian(x_k)) @ jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation:\", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        x_k1 = x_k + d_k\n",
    "        d_k1 = -la.inv(hessian(x_k)) @ jac(x_k1)\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Newton Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_modified(\n",
    "    fun,\n",
    "    x0: np.ndarray,\n",
    "    jac,\n",
    "    ls=line_search,\n",
    "    maxiter=100,\n",
    "    amax=1000.0,\n",
    "    tol=1.0e-8,\n",
    "):\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    hessian = jacobian(jac)\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    d_k = -la.inv(hessian(x_k)) @ jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation:\", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        alpha_k, _, _, _, _, success = ls(fun, jac, x_k, d_k, amax=amax)\n",
    "        if success is None:\n",
    "            print(\"Line search fail\")\n",
    "            break\n",
    "\n",
    "        print(\"alpha_k = \", alpha_k)\n",
    "        if abs(alpha_k * la.norm(d_k)) < x_eps:\n",
    "            print(\"change of x is within tolerence\")\n",
    "            break\n",
    "        x_k1 = x_k + alpha_k * d_k\n",
    "        d_k1 = -la.inv(hessian(x_k)) @ jac(x_k1)\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Davidon-Fletcher-Powell Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def davidon_fletcher_powell(\n",
    "    fun,\n",
    "    x0: np.ndarray,\n",
    "    jac,\n",
    "    H1: np.ndarray,\n",
    "    ls=line_search,\n",
    "    maxiter=100,\n",
    "    amax=1000.0,\n",
    "    tol=1.0e-8,\n",
    "):\n",
    "    if not symetry(H1) or not positive_definite(H1):\n",
    "        raise ValueError(\"H1 must be a symetric positive definite matrix\")\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    hessian = jacobian(jac)\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    H_k = H1.copy()\n",
    "    d_k = -H_k @ jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation:\", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        print(f\"H_{nit} = \", H_k)\n",
    "        alpha_k, _, _, _, _, success = ls(fun, jac, x_k, d_k, amax=amax)\n",
    "        if nit == 2:\n",
    "            alpha_k = 5 / 6\n",
    "        if success is None:\n",
    "            print(\"Line search fail\")\n",
    "            break\n",
    "\n",
    "        print(f\"alpha_{nit} = \", alpha_k)\n",
    "        if abs(alpha_k * la.norm(d_k)) < x_eps:\n",
    "            print(\"change of x is within tolerence\")\n",
    "            break\n",
    "        x_k1 = x_k + alpha_k * d_k\n",
    "        p_k = alpha_k * d_k\n",
    "        q_k = jac(x_k1) - jac(x_k)\n",
    "        H_k1 = (\n",
    "            H_k\n",
    "            + np.outer(p_k, p_k) / (p_k.T @ q_k)\n",
    "            - np.outer(H_k @ q_k, H_k @ q_k) / (q_k.T @ H_k @ q_k)\n",
    "        )\n",
    "        d_k1 = -H_k1 @ jac(x_k1)\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "        print(f\"p_{nit}\", p_k)\n",
    "        print(f\"q_{nit}\", q_k)\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1\n",
    "        H_k = H_k1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
