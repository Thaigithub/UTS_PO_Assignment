{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import numpy.linalg as la\n",
    "import autograd.numpy as np\n",
    "from autograd import grad, jacobian\n",
    "from scipy.optimize import line_search, fsolve, linprog\n",
    "from fractions import Fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class test 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix definiteness\n",
    "\n",
    "- positive (semi-positive) definite : all eigenvalue positive (non-negative)\n",
    "- negative (semi-negative) definite : all eigenvalue negative (non-possible)\n",
    "- indefinite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def positive_definite(A):\n",
    "    \"\"\"\n",
    "    Check if the matrix A is positive definite.\n",
    "    \"\"\"\n",
    "    return np.all(la.eigvals(A) > 0)\n",
    "\n",
    "\n",
    "def negative_definite(A):\n",
    "    \"\"\"\n",
    "    Check if the matrix A is positive definite.\n",
    "    \"\"\"\n",
    "    return np.all(la.eigvals(A) < 0)\n",
    "\n",
    "\n",
    "def semi_positive_definite(A):\n",
    "    \"\"\"\n",
    "    Check if the matrix A is positive definite.\n",
    "    \"\"\"\n",
    "    return np.all(la.eigvals(A) >= 0)\n",
    "\n",
    "\n",
    "def semi_negative_definite(A):\n",
    "    \"\"\"\n",
    "    Check if the matrix A is positive definite.\n",
    "    \"\"\"\n",
    "    return np.all(la.eigvals(A) <= 0)\n",
    "\n",
    "\n",
    "A = np.array([[3, 1, 5], [1, 4, 2], [5, 2, 1]])\n",
    "\n",
    "\n",
    "positive_definite(A)  # Check if A is positive definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix symetry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def symetry(A: np.ndarray):\n",
    "    \"\"\"\n",
    "    Check if the matrix A is positive definite.\n",
    "    \"\"\"\n",
    "    if len(A.shape) != 2:\n",
    "        raise False\n",
    "    if A.shape[0] != A.shape[1]:\n",
    "        raise False\n",
    "    return np.allclose(A, A.T)\n",
    "\n",
    "\n",
    "A = np.array([[3, 1, 5], [1, 4, 2], [5, 2, 1]])\n",
    "symetry(A)  # Check if A is positive definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix inverse\n",
    "\n",
    "A square matrix is invertible if\n",
    "\n",
    "- determinant is not zero\n",
    "- SVD?\n",
    "- Eigenvalue?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-80.99999999999996)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-differentiation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " gradient =  [ 0. -3.] \n",
      " Hessian =  [[2. 1.]\n",
      " [1. 2.]] \n",
      " determinant =  2.9999999999999996\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return np.sum(x**2) + x[0] * x[1] - x[0] - 2 * x[1]\n",
    "\n",
    "\n",
    "grad_f = grad(f)\n",
    "hessen_f = jacobian(grad_f)\n",
    "x = np.array([1.0, -1.0])\n",
    "print(\n",
    "    \" gradient = \",\n",
    "    grad_f(x),\n",
    "    \"\\n Hessian = \",\n",
    "    hessen_f(x),\n",
    "    \"\\n determinant = \",\n",
    "    la.det(hessen_f(x)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest Descent Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(\n",
    "    fun, x0: np.ndarray, jac, ls=line_search, maxiter=100, amax=1000.0, tol=1.0e-8\n",
    "):\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    d_k = -jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation: \", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        alpha_k, _, _, _, _, success = ls(fun, jac, x_k, d_k, amax=amax)\n",
    "        if success is None:\n",
    "            print(\"Line search fail\")\n",
    "            break\n",
    "\n",
    "        print(\"alpha_k = \", alpha_k)\n",
    "        if abs(alpha_k * la.norm(jac(x_k))) < x_eps:\n",
    "            print(\"change of x is within tolerence\")\n",
    "            break\n",
    "\n",
    "        x_k1 = x_k + alpha_k * d_k\n",
    "        d_k1 = -jac(x_k1)\n",
    "\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fletcher-Reeves Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fletcher_reeves(\n",
    "    fun, x0: np.ndarray, jac, ls=line_search, maxiter=100, amax=1000.0, tol=1.0e-8\n",
    "):\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    d_k = -jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation: \", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        alpha_k, _, _, _, _, success = ls(fun, jac, x_k, d_k, amax=amax)\n",
    "        if success is None:\n",
    "            print(\"Line search fail\")\n",
    "            break\n",
    "\n",
    "        print(\"alpha_k = \", alpha_k)\n",
    "        if abs(alpha_k * la.norm(jac(x_k))) < x_eps:\n",
    "            print(\"change of x is within tolerence\")\n",
    "            break\n",
    "\n",
    "        x_k1 = x_k + alpha_k * d_k\n",
    "        d_k1 = -jac(x_k1) + (la.norm(jac(x_k1)) ** 2 / la.norm(jac(x_k)) ** 2) * d_k\n",
    "\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fletcher-Reeves Reset Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fletcher_reeves_reset(\n",
    "    fun,\n",
    "    x0: np.ndarray,\n",
    "    jac,\n",
    "    ls=line_search,\n",
    "    maxiter=100,\n",
    "    amax=1000.0,\n",
    "    tol=1.0e-8,\n",
    "    reset=100,\n",
    "):\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    d_k = -jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation:\", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        alpha_k, _, _, _, _, success = ls(fun, jac, x_k, d_k, amax=amax)\n",
    "        if success is None:\n",
    "            print(\"Line search fail\")\n",
    "            break\n",
    "\n",
    "        print(\"alpha_k = \", alpha_k)\n",
    "        if abs(alpha_k * la.norm(jac(x_k))) < x_eps:\n",
    "            print(\"change of x is within tolerence\")\n",
    "            break\n",
    "        x_k1 = x_k + alpha_k * d_k\n",
    "        d_k1 = None\n",
    "        if nit % reset == 0:\n",
    "            d_k1 = -jac(x_k1)\n",
    "        else:\n",
    "            d_k1 = -jac(x_k1) + (la.norm(jac(x_k1)) ** 2 / la.norm(jac(x_k)) ** 2) * d_k\n",
    "\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(\n",
    "    fun,\n",
    "    x0: np.ndarray,\n",
    "    jac,\n",
    "    ls=line_search,\n",
    "    maxiter=100,\n",
    "    amax=1000.0,\n",
    "    tol=1.0e-8,\n",
    "):\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    hessian = jacobian(jac)\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    d_k = -la.inv(hessian(x_k)) @ jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation:\", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        x_k1 = x_k + d_k\n",
    "        d_k1 = -la.inv(hessian(x_k)) @ jac(x_k1)\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Newton Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_modified(\n",
    "    fun,\n",
    "    x0: np.ndarray,\n",
    "    jac,\n",
    "    ls=line_search,\n",
    "    maxiter=100,\n",
    "    amax=1000.0,\n",
    "    tol=1.0e-8,\n",
    "):\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    hessian = jacobian(jac)\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    d_k = -la.inv(hessian(x_k)) @ jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation:\", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        alpha_k, _, _, _, _, success = ls(fun, jac, x_k, d_k, amax=amax)\n",
    "        if success is None:\n",
    "            print(\"Line search fail\")\n",
    "            break\n",
    "\n",
    "        print(\"alpha_k = \", alpha_k)\n",
    "        if abs(alpha_k * la.norm(d_k)) < x_eps:\n",
    "            print(\"change of x is within tolerence\")\n",
    "            break\n",
    "        x_k1 = x_k + alpha_k * d_k\n",
    "        d_k1 = -la.inv(hessian(x_k)) @ jac(x_k1)\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Davidon-Fletcher-Powell Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def davidon_fletcher_powell(\n",
    "    fun,\n",
    "    x0: np.ndarray,\n",
    "    jac,\n",
    "    H1: np.ndarray,\n",
    "    ls=line_search,\n",
    "    maxiter=100,\n",
    "    amax=1000.0,\n",
    "    tol=1.0e-8,\n",
    "):\n",
    "    if not symetry(H1) or not positive_definite(H1):\n",
    "        raise ValueError(\"H1 must be a symetric positive definite matrix\")\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    hessian = jacobian(jac)\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    H_k = H1.copy()\n",
    "    d_k = -H_k @ jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation:\", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        print(f\"H_{nit} = \", H_k)\n",
    "        alpha_k, _, _, _, _, success = ls(fun, jac, x_k, d_k, amax=amax)\n",
    "        if nit == 2:\n",
    "            alpha_k = 5 / 6\n",
    "        if success is None:\n",
    "            print(\"Line search fail\")\n",
    "            break\n",
    "\n",
    "        print(f\"alpha_{nit} = \", alpha_k)\n",
    "        if abs(alpha_k * la.norm(d_k)) < x_eps:\n",
    "            print(\"change of x is within tolerence\")\n",
    "            break\n",
    "        x_k1 = x_k + alpha_k * d_k\n",
    "        p_k = alpha_k * d_k\n",
    "        q_k = jac(x_k1) - jac(x_k)\n",
    "        H_k1 = (\n",
    "            H_k\n",
    "            + np.outer(p_k, p_k) / (p_k.T @ q_k)\n",
    "            - np.outer(H_k @ q_k, H_k @ q_k) / (q_k.T @ H_k @ q_k)\n",
    "        )\n",
    "        d_k1 = -H_k1 @ jac(x_k1)\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "        print(f\"p_{nit}\", p_k)\n",
    "        print(f\"q_{nit}\", q_k)\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1\n",
    "        H_k = H_k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class test 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printer(x: np.ndarray):\n",
    "    \"\"\"\n",
    "    Print the value of x\n",
    "    \"\"\"\n",
    "    if len(x.shape) == 2:\n",
    "        for i in range(len(x)):\n",
    "            print(\n",
    "                [f\"{Fraction(x[i, j]).limit_denominator()}\" for j in range(len(x[i]))]\n",
    "            )\n",
    "    else:\n",
    "        print([f\"{Fraction(x[i]).limit_denominator()}\" for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced Gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basis_selection(arr, n):\n",
    "    sorted_indices = np.argsort(arr)  # Ascending order\n",
    "    largest_indices = sorted_indices[-n:][::-1]  # n largest, descending\n",
    "    rest_indices = sorted_indices[:-n]  # The rest, still ascending\n",
    "    largest_indices.sort()\n",
    "    rest_indices.sort()\n",
    "    return largest_indices, rest_indices\n",
    "\n",
    "\n",
    "def reduce_gradient(\n",
    "    fun, x0: np.ndarray, A: np.ndarray, b: np.ndarray, maxiter=100, tol=1.0e-8\n",
    "):\n",
    "    jac = grad(fun)\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    nit = 1\n",
    "    x = x0.copy()\n",
    "    while True:\n",
    "        print(\"______________________________\")\n",
    "        print(\"ITERATION:\", nit)\n",
    "        print(\"-----DIRECTION SEARCH-----\")\n",
    "        basis_index, non_index = basis_selection(x, m)\n",
    "        print(\"basis_index =\", basis_index + 1)\n",
    "        xb = x[basis_index]\n",
    "        xn = x[non_index]\n",
    "        gra = jac(x)\n",
    "        B = A[:, basis_index]\n",
    "        print(\"B =\")\n",
    "        printer(B)\n",
    "        print(\"B-1 =\")\n",
    "        printer(la.inv(B))\n",
    "        N = A[:, non_index]\n",
    "        print(\"N =\")\n",
    "        printer(N)\n",
    "        gb = gra[basis_index]\n",
    "        print(\"g =\")\n",
    "        printer(jac(x))\n",
    "        print(\"gb =\")\n",
    "        printer(gb)\n",
    "        gn = gra[non_index]\n",
    "        print(\"gn =\")\n",
    "        printer(gn)\n",
    "        rn = gn - gb.T @ la.inv(B) @ N\n",
    "        print(\"rn =\")\n",
    "        printer(rn)\n",
    "        dn = np.zeros(n - m)\n",
    "        for i in range(n - m):\n",
    "            if rn[i] <= 0:\n",
    "                dn[i] = -rn[i]\n",
    "            else:\n",
    "                dn[i] = -xn[i] * rn[i]\n",
    "        print(\"dn =\")\n",
    "        printer(dn)\n",
    "        if np.all(dn == 0):\n",
    "            print(\"Optimization terminated\")\n",
    "            break\n",
    "        db = -la.inv(B) @ N @ dn\n",
    "        print(\"db =\")\n",
    "        printer(db)\n",
    "        dk = np.zeros(n)\n",
    "        b_count = 0\n",
    "        n_count = 0\n",
    "        for i in range(n):\n",
    "            if i in basis_index:\n",
    "                dk[i] = db[b_count]\n",
    "                b_count += 1\n",
    "            else:\n",
    "                dk[i] = dn[n_count]\n",
    "                n_count += 1\n",
    "        print(\"dk =\")\n",
    "        printer(dk)\n",
    "        bounds = [0, np.inf]\n",
    "        for i in range(n):\n",
    "            if dk[i] > 0:\n",
    "                bc = -x[i] / dk[i]\n",
    "                if bc > bounds[0]:\n",
    "                    bounds[0] = bc\n",
    "            elif dk[i] < 0:\n",
    "                bc = -x[i] / dk[i]\n",
    "                if bc < bounds[1]:\n",
    "                    bounds[1] = bc\n",
    "        print(\"-----LINE SEARCH-----\")\n",
    "        print(\"bounds =\")\n",
    "        printer(np.array(bounds))\n",
    "        ls = lambda alpha: fun(x + alpha * dk)\n",
    "        gs = grad(ls)\n",
    "        a = fsolve(gs, 1)[0]\n",
    "        if a < bounds[0]:\n",
    "            a = bounds[0]\n",
    "        elif a > bounds[1]:\n",
    "            a = bounds[1]\n",
    "        print(\"alpha =\", str(Fraction(a).limit_denominator()))\n",
    "        x = x + a * dk\n",
    "        print(\"x =\")\n",
    "        printer(x)\n",
    "        nit += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_projection(\n",
    "    fun,\n",
    "    x0: np.ndarray,\n",
    "    A: np.ndarray,\n",
    "    b: np.ndarray,\n",
    "    W: np.ndarray = None,\n",
    "    w: np.ndarray = None,\n",
    "    maxiter=100,\n",
    "    tol=1.0e-8,\n",
    "):\n",
    "    jac = grad(fun)\n",
    "    nit = 1\n",
    "    x = x0.copy()\n",
    "    while True:\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "        print(\"______________________________\")\n",
    "        print(\"ITERATION:\", nit)\n",
    "        print(\"-----DIRECTION SEARCH-----\")\n",
    "        print(\"grad =\")\n",
    "        printer(jac(x))\n",
    "        active_index = np.where(np.abs(A @ x - b) <= tol)[0]\n",
    "        print(\"active_index =\")\n",
    "        printer(active_index + 1)\n",
    "        A_active = A[active_index, :]\n",
    "        print(\"A_active =\")\n",
    "        printer(A_active)\n",
    "        b_active = b[active_index]\n",
    "        print(\"b_active =\")\n",
    "        printer(b_active)\n",
    "        inactive_index = np.setdiff1d(np.arange(A.shape[0]), active_index)\n",
    "        A_non_active = A[inactive_index, :]\n",
    "        b_non_active = b[inactive_index]\n",
    "        M = A_active\n",
    "        dk = None\n",
    "        if W is not None:\n",
    "            M = np.vstack((A_active, W))\n",
    "        print(\"M =\")\n",
    "        printer(M)\n",
    "        # print(\"M @ M.T =\")\n",
    "        # printer(M @ M.T)\n",
    "        # print(\"(M @ M.T)^-1 =\")\n",
    "        # printer(la.inv(M @ M.T))\n",
    "        # print(\"((M @ M.T)^-1)@M =\")\n",
    "        # printer(la.inv(M @ M.T) @ M)\n",
    "        # print(\"M.T @ ((M @ M.T)^-1) @ M =\")\n",
    "        # printer(M.T @ la.inv(M @ M.T) @ M)\n",
    "        while True:\n",
    "            if len(M) == 0:\n",
    "                if np.all(np.abs(jac(x)) <= tol):\n",
    "                    print(\"grad =\", 0)\n",
    "                    print(\"Optimization terminated due to gradient = 0\")\n",
    "                    return None\n",
    "                else:\n",
    "                    dk = -jac(x)\n",
    "                    print(\"step 4 dk =\")\n",
    "                    printer(dk)\n",
    "                    break\n",
    "            else:\n",
    "                dk = -(np.eye(M.shape[1]) - M.T @ la.inv(M @ M.T) @ M) @ jac(x)\n",
    "                print(\"dk =\")\n",
    "                printer(dk)\n",
    "                if np.all(np.abs(dk) <= tol):\n",
    "                    dk = np.zeros(len(dk))\n",
    "                    laragian = -la.inv(M @ M.T) @ M @ jac(x)\n",
    "                    print(\"Lagrangian =\")\n",
    "                    printer(laragian)\n",
    "                    muy = laragian[: len(A)]\n",
    "                    # print(\"muy =\")\n",
    "                    # printer(muy)\n",
    "                    if np.all(muy >= 0):\n",
    "                        print(\"Optimization terminated due to muy >= 0\")\n",
    "                        return None\n",
    "                    else:\n",
    "                        remove_index = np.where(muy < 0)[0][0]\n",
    "                        M = np.delete(M, remove_index, axis=0)\n",
    "                        print(\"Modified matrix =\")\n",
    "                        printer(M)\n",
    "                        # print(\"M @ M.T =\")\n",
    "                        # printer(M @ M.T)\n",
    "                        # print(\"(M @ M.T)^-1 =\")\n",
    "                        # printer(la.inv(M @ M.T))\n",
    "                        # print(\"((M @ M.T)^-1)@M =\")\n",
    "                        # printer(la.inv(M @ M.T) @ M)\n",
    "                        # print(\"M.T @ ((M @ M.T)^-1) @ M =\")\n",
    "                        # printer(M.T @ la.inv(M @ M.T) @ M)\n",
    "                else:\n",
    "                    break\n",
    "        print(\"-----LINE SEARCH-----\")\n",
    "        RHS = b_non_active - A_non_active @ x\n",
    "        # print(\"RHS =\")\n",
    "        # printer(RHS)\n",
    "        LHS = A_non_active @ dk\n",
    "        # print(\"LHS =\")\n",
    "        # printer(LHS)\n",
    "        bounds = [0, np.inf]\n",
    "        for i in range(len(LHS)):\n",
    "            if LHS[i] > 0:\n",
    "                bc = RHS[i] / LHS[i]\n",
    "                if bc < bounds[1]:\n",
    "                    bounds[1] = bc\n",
    "            elif LHS[i] < 0:\n",
    "                bc = RHS[i] / LHS[i]\n",
    "                if bc > bounds[0]:\n",
    "                    bounds[0] = bc\n",
    "        a_max = max(bounds[0], bounds[1], 0)\n",
    "        # print(\"a_max =\", str(Fraction(a_max).limit_denominator()))\n",
    "        bounds = [0, a_max]\n",
    "        print(\"bounds =\")\n",
    "        printer(np.array(bounds))\n",
    "        ls = lambda alpha: fun(x + alpha * dk)\n",
    "        gs = grad(ls)\n",
    "        a = fsolve(gs, 1)[0]\n",
    "        if a < bounds[0]:\n",
    "            a = bounds[0]\n",
    "        elif a > bounds[1]:\n",
    "            a = bounds[1]\n",
    "        print(\"alpha =\", str(Fraction(a).limit_denominator()))\n",
    "        x = x + a * dk\n",
    "        print(\"x =\")\n",
    "        printer(x)\n",
    "        nit += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of Zoutendijk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoutendijk(\n",
    "    fun,\n",
    "    x0: np.ndarray,\n",
    "    A: np.ndarray,\n",
    "    b: np.ndarray,\n",
    "    W: np.ndarray = None,\n",
    "    w: np.ndarray = None,\n",
    "    maxiter=100,\n",
    "    tol=1.0e-8,\n",
    "):\n",
    "    jac = grad(fun)\n",
    "    nit = 1\n",
    "    x = x0.copy()\n",
    "    n = len(x0)\n",
    "    while True:\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "        print(\"______________________________\")\n",
    "        print(\"ITERATION:\", nit)\n",
    "        print(\"-----DIRECTION SEARCH-----\")\n",
    "        gra = jac(x)\n",
    "        print(\"grad =\")\n",
    "        printer(gra)\n",
    "        active_index = np.where(np.abs(A @ x - b) <= tol)[0]\n",
    "        print(\"active_index =\")\n",
    "        printer(active_index + 1)\n",
    "        A_active = A[active_index, :]\n",
    "        print(\"A_active =\")\n",
    "        printer(A_active)\n",
    "        b_active = b[active_index]\n",
    "        print(\"b_active =\")\n",
    "        printer(b_active)\n",
    "        inactive_index = np.setdiff1d(np.arange(A.shape[0]), active_index)\n",
    "        A_non_active = A[inactive_index, :]\n",
    "        print(\"A_non_active =\")\n",
    "        printer(A_non_active)\n",
    "        b_non_active = b[inactive_index]\n",
    "        print(\"b_non_active =\")\n",
    "        printer(b_non_active)\n",
    "        dk = np.array(\n",
    "            linprog(\n",
    "                gra,\n",
    "                A_ub=A_active,\n",
    "                b_ub=np.zeros(len(A_active)),\n",
    "                bounds=[(-1, 1), (-1, 1)],\n",
    "                method=\"highs\",\n",
    "            ).x\n",
    "        )\n",
    "        print(\"dk =\")\n",
    "        printer(dk)\n",
    "        if np.abs(gra.T @ dk) <= tol:\n",
    "            print(\"Optimization terminated due to grad * dk = 0\")\n",
    "            break\n",
    "        print(\"-----LINE SEARCH-----\")\n",
    "        RHS = b_non_active - A_non_active @ x\n",
    "        print(\"RHS =\")\n",
    "        printer(RHS)\n",
    "        LHS = A_non_active @ dk\n",
    "        print(\"LHS =\")\n",
    "        printer(LHS)\n",
    "        bounds = [0, np.inf]\n",
    "        for i in range(len(LHS)):\n",
    "            if LHS[i] > 0:\n",
    "                bc = RHS[i] / LHS[i]\n",
    "                if bc < bounds[1]:\n",
    "                    bounds[1] = bc\n",
    "            elif LHS[i] < 0:\n",
    "                bc = RHS[i] / LHS[i]\n",
    "                if bc > bounds[0]:\n",
    "                    bounds[0] = bc\n",
    "        a_max = max(bounds[0], bounds[1], 0)\n",
    "        print(\"a_max =\", str(Fraction(a_max).limit_denominator()))\n",
    "        bounds = [0, a_max]\n",
    "        print(\"bounds =\")\n",
    "        printer(np.array(bounds))\n",
    "        ls = lambda alpha: fun(x + alpha * dk)\n",
    "        gs = grad(ls)\n",
    "        a = fsolve(gs, 1)[0]\n",
    "        if a < bounds[0]:\n",
    "            a = bounds[0]\n",
    "        elif a > bounds[1]:\n",
    "            a = bounds[1]\n",
    "\n",
    "        if nit == 2:\n",
    "            a = 5\n",
    "        print(\"alpha =\", str(Fraction(a).limit_denominator()))\n",
    "        x = x + a * dk\n",
    "        print(\"x =\")\n",
    "        printer(x)\n",
    "        nit += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________\n",
      "ITERATION: 1\n",
      "-----DIRECTION SEARCH-----\n",
      "grad =\n",
      "['2', '1']\n",
      "active_index =\n",
      "[]\n",
      "A_active =\n",
      "b_active =\n",
      "[]\n",
      "A_non_active =\n",
      "['-1', '3']\n",
      "['2', '1']\n",
      "['-1', '-1']\n",
      "b_non_active =\n",
      "['8', '4', '1']\n",
      "dk =\n",
      "['-1', '-1']\n",
      "-----LINE SEARCH-----\n",
      "RHS =\n",
      "['9', '2', '2']\n",
      "LHS =\n",
      "['-2', '-3', '2']\n",
      "a_max = 1\n",
      "bounds =\n",
      "['0', '1']\n",
      "alpha = 1\n",
      "x =\n",
      "['0', '-1']\n",
      "______________________________\n",
      "ITERATION: 2\n",
      "-----DIRECTION SEARCH-----\n",
      "grad =\n",
      "['-1', '2']\n",
      "active_index =\n",
      "['3']\n",
      "A_active =\n",
      "['-1', '-1']\n",
      "b_active =\n",
      "['1']\n",
      "A_non_active =\n",
      "['-1', '3']\n",
      "['2', '1']\n",
      "b_non_active =\n",
      "['8', '4']\n",
      "dk =\n",
      "['1', '-1']\n",
      "-----LINE SEARCH-----\n",
      "RHS =\n",
      "['11', '5']\n",
      "LHS =\n",
      "['-4', '1']\n",
      "a_max = 5\n",
      "bounds =\n",
      "['0', '5']\n",
      "alpha = 5\n",
      "x =\n",
      "['5', '-6']\n",
      "______________________________\n",
      "ITERATION: 3\n",
      "-----DIRECTION SEARCH-----\n",
      "grad =\n",
      "['4', '17']\n",
      "active_index =\n",
      "['2', '3']\n",
      "A_active =\n",
      "['2', '1']\n",
      "['-1', '-1']\n",
      "b_active =\n",
      "['4', '1']\n",
      "A_non_active =\n",
      "['-1', '3']\n",
      "b_non_active =\n",
      "['8']\n",
      "dk =\n",
      "['0', '0']\n",
      "Optimization terminated due to grad * dk = 0\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: x[0] ** 2 - x[1] ** 2 + x[0] * x[1]\n",
    "A = np.array([[-1, 3], [2, 1], [-1, -1]])\n",
    "b = np.array([8, 4, 1])\n",
    "x0 = np.array([1.0, 0.0])\n",
    "zoutendijk(f, x0, A, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
