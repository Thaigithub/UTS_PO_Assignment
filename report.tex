\documentclass[12pt]{article}


% This can be used if you want to increase the spacing between lines to 1.5x
\linespread{1.5}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{rotating}
\usepackage{booktabs}
\usepackage[round]{natbib}
\geometry{a4paper, margin=1in}
\setlength{\parskip}{0.8em}
\setlength{\parindent}{0pt}
\begin{document}
\title{Mathematical Formulation of Nelson-Siegel Model Calibration via Loss-Based Optimization}
\maketitle



\begin{center}
\section*{Student:}
\begin{tabular}{@{}ll@{}}
Hayoung Lee:   & \href{mailto:hayoung.lee-2@student.uts.edu.au}{hayoung.lee-2@student.uts.edu.au} \\ 
Quoc Thai Tran:& \href{mailto:quocthai.tran@student.uts.edu.au}{quocthai.tran@student.uts.edu.au} \\ 
Stephanie Zhen:& \href{mailto:liyee.zhen@student.uts.edu.au}{liyee.zhen@student.uts.edu.au}\\ 
Ziqi Zhou:     & \href{mailto:ziqi.zhou-3@student.uts.edu.au}{ziqi.zhou-3@student.uts.edu.au} 
\end{tabular}

\section*{Lecturer:}
Hanyu Gu: \href{mailto:hanyu.gu@uts.edu.au}{hanyu.gu@uts.edu.au}
\vfill
\large
School of Mathematical and Physical Sciences\\
Faculty of Science\\
University of Technology Sydney\\

\end{center}
\clearpage
\tableofcontents

\pagenumbering{arabic}
\vspace{1cm}
\clearpage
\section{Introduction}

The accurate estimation of the yield curve is a cornerstone of fixed income analysis, crucial for pricing bonds, managing interest rate risk, and performing monetary policy analysis. Among the various term‑structure models, the \cite{NelsonSiegel1987} (NS model) has become one of the most widely used due to its parsimonious structure, intuitive interpretation, and empirical performance.

Traditionally, the Nelson–Siegel curve is estimated by minimising the squared error between observed market yields and the model’s fitted yields. However, in practice, each quoted yield used for error minimisation is typically taken as the midpoint of a bid-ask spread which varies significantly across bonds. The width of this spread contains valuable information about the bond's liquidity, uncertainty, and transaction costs. As a result, equally weighting all mid quotes can allow illiquid bonds with wide bid-ask spreads to disproportionately influence and distort the fitted curve, which can mislead interpretations.

To address this issue, a flexible calibration framework is proposed in order to embed bid–ask information directly during curve fitting. Specifically, a hinge-type loss function is designed such that it penalizes any fitted yield that falls outside the observed bid–ask interval with a quadratic cost, while imposing no penalty for in-spread fits. This retains the contribution of well-priced mid-quotes while substantially reduces any over-fitting caused any wide or illiquid spreads.

This project formally develops the mathematical formulation of the loss-based calibration approach, implements several gradient-based and derivative-free optimisers (benchmarked against scipy.optimize, the industry standard for optimisation in Python), and evaluates performance on Reserve Bank of Australia benchmark yield data. The ultimate goal is to develop a robust, interpretable calibration method that respects market-implied price bounds and enhances the reliability of term-structure estimation.

\section{Literature Review}\label{sec:lit-review}

The Nelson--Siegel (NS) model was originally proposed by \citet{NelsonSiegel1987} as a parsimonious three-factor representation of the yield curve.  
Its ability to capture the level, slope and curvature of the term structure with just four parameters has led to widespread adoption by central banks and market participants.

Recognising that bond prices are typically quoted in the market with bid–ask spreads, \citet{GomesGoncalvesGzylMayoral2017} calibrated short-rate term structure models using intervals rather than point prices.  
By treating every coupon-bond quotation as the admissible range \([\text{bid},\text{ask}]\), it was shown that ignoring spreads systematically biases the estimated NS factors, especially for illiquid maturities. Hence, a maximum-entropy procedure was proposed to find the smoothest zero-coupon curve consistent with all intervals.

Based on this insight, \citet{LapshinSohatskaya2020} developed an optimisation scheme in which model prices are penalised {\em only when they fall outside} the observed bid–ask band.  
Several weighting and loss-function designs were experimented on and demonstrated using sovereign bond data, it was discovered that interval-aware calibration is capable of improving out-of-sample pricing accuracy and hedging stability without sacrificing parsimony.

Together, these studies underscored two key lessons:  
(i) bid–ask information contains valuable constraints that should be respected during curve fitting, and  
(ii) loss-based optimisation that tolerates any price within the trading range yields more robust NS parameter estimates.  
These ideas form the conceptual backbone of the calibration framework advanced in the present report.

\clearpage
\section{Objectives \& Scope}

\subsection{Objectives}

The primary objective of this project is to calibrate the NS yield‑curve model using a loss function that incorporates observed bid–ask spreads of coupon‑bearing bonds. Specifically, we aim to:

\begin{enumerate}
    \item Formulate a mathematically consistent loss function that penalises model‑implied prices lying outside market bid–ask bounds
    \item Derive the analytical expressions for bond pricing, discount factors and the gradient of the loss function with respect to model parameters
    \item Implement and compare multiple optimisation algorithms
    \item Analyse the numerical performance, convergence behaviour and robustness of each method
\end{enumerate}

\subsection{Scope}

This study is limited to the calibration of the NS model on a cross section of fixed coupon, default free government bonds. The scope is defined by the following assumptions and boundaries:

\begin{enumerate}
    \item Interest rates are modeled under a deterministic framework using continuously compounded yields
    \item The model ignores liquidity premiums, tax considerations and default risk
    \item Only static parameter estimation is considered—dynamic models or real‑time updating are beyond this project’s scope
    \item The optimisation allows $\gamma\geq0$. 
\end{enumerate}

\section{Methodology}\label{sec:method}

\subsection{Mathematical Formulation and Loss-Based Calibration Framework}

\subsubsection{Nelson-Siegel Yield Curve Model}

The instantaneous forward rate function is:
\[
f(t) = f_0 + f_1 e^{-t/\gamma} + f_2 \cdot \frac{t}{\gamma} e^{-t/\gamma}
\]
where the parameter vector is:
\[
\boldsymbol{\theta} = (f_0, f_1, f_2, \gamma)^\top \in \mathbb{R}^4, \quad \text{with } \gamma > 0
\]

In this model, \( f_0 \) represents the long-term level of the yield curve, \( f_1 \) controls the short-term component (slope), \( f_2 \) captures the medium-term component (curvature), and \( \gamma \) is the decay factor that determines the exponential shape of the curve.


\subsubsection{Zero-Coupon Discount Factor}

The forward rate is defined as:
\[
f(\boldsymbol{\theta}, t) = f_0 + f_1 e^{- \frac{t}{\gamma}} + f_2 \cdot \frac{t}{\gamma} e^{- \frac{t}{\gamma}}
\]
Then, the discount factor is:
\[
B(\boldsymbol{\theta}, T) = \exp\left( - \int_0^T f(\boldsymbol{\theta}, t) \, dt \right)
\]
Then,
\[
 \int_0^T f(\boldsymbol{\theta}, t) \, dt =
\int_0^T \left( f_0 + f_1 e^{- \frac{t}{\gamma}} + f_2 \cdot \frac{t}{\gamma} e^{- \frac{t}{\gamma}} \right) dt
= T f_0 + \left( \gamma - e^{- \frac{T}{\gamma}} \gamma \right) f_1 + \left( \gamma - e^{- \frac{T}{\gamma}} (T + \gamma) \right) f_2
\]

Hence,
\[
B(\boldsymbol{\theta}, T)= \exp\left[
- T f_0
- \left( \gamma - e^{- \frac{T}{\gamma}} \gamma \right) f_1
- \left( \gamma - e^{- \frac{T}{\gamma}} (T + \gamma) \right) f_2
\right]
\]

\subsubsection{Bond Pricing with Coupons}\label{subsec:bond_pricing}

The price of bond \( j \) with \( n_j \) coupon payments is given by:
\[
V_j = 100 \left( \sum_{i=1}^{n_j } c_j \cdot B(\boldsymbol{\theta}, T_{i}) +  B(\boldsymbol{\theta}, T_{n_j}) \right)
\]

\subsubsection{Market Bid-Ask Bound Constraints}

Let the observed market bid and ask prices of bond \( j \) be:
\[
\text{Bid}_j, \quad \text{Ask}_j, \quad \text{with } \text{Bid}_j \le \text{Ask}_j
\]

We aim for \( V_j(\theta) \in [\text{Bid}_j, \text{Ask}_j] \) to reflect realistic market pricing.

\subsubsection{Loss Function Construction}

We define a penalty-based loss function for bond \( j \) as:
\[
\ell_j(\theta) =
\left( \frac{\max(0, \text{Bid}_j - V_j(\theta))}{\text{Bid}_j} \right)^2 +
\left( \frac{\max(0, V_j(\theta) - \text{Ask}_j)}{\text{Ask}_j} \right)^2
\]

This function penalizes the model only when \( V_j(\theta) \notin [\text{Bid}_j, \text{Ask}_j]  \).

\subsubsection{Total Loss Function}

The total loss across all \( m \) bonds is:
\[
L(\theta) = \sum_{j=1}^m \ell_j(\theta)
\]

\subsubsection{Optimization Objective}

The loss function is continuous and differentiable almost everywhere with respect to the parameter vector \( \boldsymbol{\theta} = (f_0, f_1, f_2, \gamma)^\top \) with the constraint \( \gamma \ge 0 \); expressions are evaluated for $\gamma>0$ and extended by continuity at $\gamma=0$. 

We solve the following optimization problem:
\[
\min_{f_0, f_1, f_2, \gamma} \left(
\sum_{j=1}^m \left[
\left( \frac{\max(0, \text{Bid}_j - V_j)}{{Bid}_j} \right)^2 +
\left( \frac{\max(0, V_j - \text{Ask}_j)}{\text{Ask}_j} \right)^2
\right]
\right)
\]

\subsubsection{Gradient and Hessian Calculation}\label{subsubsec:grad_hess}

\textbf{Analytical}

The first and second-order derivatives of the hinge-type loss function were derived analytically and implemented manually in code. It is continuously differentiable \( C^1 \), but not twice continuously differentiable \( C^2 \) (the second derivative exists piece-wise but is discontinuous at the bid-ask boundaries, although its behaviour can be approximated in the numerical section below). This level of smoothness is sufficient for standard gradient-based optimisation algorithms, although it involves inequality constraints and non‑linearity due to the exponential structure of the NS model. Detailed derivations can be found in Appendix A. 

\textbf{Numerical}

Choose a small step \(\varepsilon>0\) and let \(\mathbf e_k\in\mathbb R^{4}\) denote the \(k\)-th standard basis vector.  For the total loss
\[
  L(\boldsymbol{\theta})=\sum_{j=1}^{m}
  \Bigl(\tfrac{\max\!\bigl(0,\,V_j(\boldsymbol{\theta})-A_j\bigr)}{A_j}\Bigr)^{2}
 \;+\;
  \Bigl(\tfrac{\max\!\bigl(0,\,B_j-V_j(\boldsymbol{\theta})\bigr)}{B_j}\Bigr)^{2}.
  \text{  } B_j = Bid_j\text{  ,  } A_j = Ask_j
\]
the \(k\)-th partial derivative is numerically approximated by the central-difference formula  
\[
  \frac{\partial L}{\partial\theta_k}(\boldsymbol{\theta})
  \;\approx\;
  \frac{L(\boldsymbol{\theta}+\varepsilon\mathbf e_k)\;-\;L(\boldsymbol{\theta}-\varepsilon\mathbf e_k)}
       {2\,\varepsilon}.
\]
Defining the perturbed vectors \(\boldsymbol{\theta}^{(+)}_k = \boldsymbol{\theta}+\varepsilon\mathbf e_k\) and \(\boldsymbol{\theta}^{(-)}_k = \boldsymbol{\theta}-\varepsilon\mathbf e_k\), this may be rewritten component-wise as  
\[
  \frac{\partial L}{\partial\theta_k}(\boldsymbol{\theta})
  \;\approx\;
  \sum_{j=1}^{m}
  \frac{L_j\bigl(\boldsymbol{\theta}^{(+)}_k\bigr)
        -L_j\bigl(\boldsymbol{\theta}^{(-)}_k\bigr)}
       {2\,\varepsilon},
\]
If one prefers to approximate the derivative of each bond value first, the same scheme yields  
\[
  \frac{\partial V_j}{\partial\theta_k}(\boldsymbol{\theta})
  \;\approx\;
  \frac{V_j(\boldsymbol{\theta}^{(+)}_k)-V_j(\boldsymbol{\theta}^{(-)}_k)}{2\,\varepsilon},
  \qquad
  \frac{\partial B(\boldsymbol{\theta},T)}{\partial\theta_k}
  \;\approx\;
  \frac{B(\boldsymbol{\theta}^{(+)}_k,T)-B(\boldsymbol{\theta}^{(-)}_k,T)}{2\,\varepsilon}.
\]

Denote by \(\mathbf e_k,\mathbf e_\ell\in\mathbb R^{4}\) the standard basis vectors indexed by the two integers \(k,\ell\in\{0,1,2,3\}\) (\(k\le\ell\) after sorting).  
Define the four perturbed parameter vectors
\[
\begin{aligned}
\boldsymbol{\theta}^{(++)}&=\boldsymbol{\theta}+\varepsilon\mathbf e_k+\varepsilon\mathbf e_\ell,\\
\boldsymbol{\theta}^{(+-)}&=\boldsymbol{\theta}+\varepsilon\mathbf e_k-\varepsilon\mathbf e_\ell,\\
\boldsymbol{\theta}^{(-+)}&=\boldsymbol{\theta}-\varepsilon\mathbf e_k+\varepsilon\mathbf e_\ell,\\
\boldsymbol{\theta}^{(--)}&=\boldsymbol{\theta}-\varepsilon\mathbf e_k-\varepsilon\mathbf e_\ell,
\end{aligned}
\]
and evaluate the loss at these points:
\(L^{(++)}=L(\boldsymbol{\theta}^{(++)}),\;
  L^{(+-)}=L(\boldsymbol{\theta}^{(+-)}),\;
  L^{(-+)}=L(\boldsymbol{\theta}^{(-+)}),\;
  L^{(--)}=L(\boldsymbol{\theta}^{(--)})\).
  
The mixed second-order partial derivative is then approximated by the central finite-difference formula
\[
\frac{\partial^{2}L}{\partial\theta_k\partial\theta_\ell}(\boldsymbol{\theta})
\;\approx\;
\frac{L^{(++)}-L^{(+-)}-L^{(-+)}+L^{(--)}}{4\varepsilon^{2}}
\].

%---------------
\subsection{Property of the loss function}
\label{sec:property}
%---------------
The penalised objective is:
\[
L(\boldsymbol{\theta})
=\sum_{j=1}^{m}
  \Bigl[\tfrac{\max(0,\text{B}_{j}-V_{j}(\boldsymbol{\theta}))}
              {\text{B}_{j}}\Bigr]^{2}
 +\Bigl[\tfrac{\max(0,V_{j}(\boldsymbol{\theta})-\text{A}_{j})}
              {\text{A}_{j}}\Bigr]^{2},
\qquad 
\boldsymbol{\theta}=(f_0,f_1,f_2,\gamma)^{\!\top},
\]
where each model price  \(V_{j}(\boldsymbol{\theta})=100\bigl[c_j\sum_{i}B(\boldsymbol{\theta},T_{ji})
                               +B(\boldsymbol{\theta},T_{jn_j})\bigr]\) is non-linear in \(\boldsymbol{\theta}\) through the discount factor
\[
B(\boldsymbol{\theta},T)=
\exp\!\Bigl\{-Tf_0-(\gamma-e^{-T/\gamma}\gamma)f_1 -
          \bigl[\gamma-e^{-T/\gamma}(T+\gamma)\bigr]f_2\Bigr\},
\qquad \gamma\geq0.
\]

\subsubsection{Differentiability of \(L\)}
As shown in \S\ref{subsubsec:grad_hess}, the penalised loss \(L(\theta)\) is continuously differentiable (\(C^{1}\)); its Hessian exists piece-wise but is discontinuous at the bid–ask boundaries.  Hence analytic expressions (or stable finite–difference approximations) are available for both the gradient and the Hessian.  This level of smoothness is sufficient for the use of first-order methods (steepest-descent, conjugate-gradient) as well as Newton-type or quasi-Newton algorithms (e.g.\ BFGS, L-BFGS-B, trust-region Newton) that can accommodate a piece-wise continuous Hessian.


\subsubsection{Non-convexity of \(L\)}
A twice–differentiable function is \emph{strictly} convex only if its Hessian is positive-definite everywhere.  
To test this condition empirically, the eigenvalues of \(\nabla^{2}L(\theta)\) were evaluated on a four–dimensional grid  
\[
f_0,f_1,f_2\in[-5,5],\qquad
\gamma\in(0,20],
\]
which covers the parameter region of practical interest for our model.
For each grid point the Hessian was assembled analytically and its eigenvalues computed with NumPy’s \texttt{eigvalsh}.  
Throughout the grid, the Hessian has always yielded non positive eigenvalues, failing to be positive-definite.  
Therefore \(L\) is non-convex on the domain explored, and any solution returned by an optimiser can only be regarded as a \emph{local} minimum. 

\subsubsection{Optimisation Experiments}
The loss function was first minimised with \verb|scipy.optimize| using the quasi-Newton BFGS algorithm, chosen for its robustness on mildly non-convex problems. Through experimentation, however, BFGS was sensitive to the starting point: several initial guesses failed to converge or terminated at implausible local minima. To concentrate on the model relevant region of the parameter space, the initial vector was fixed at
\[
x_{0}
=\begin{bmatrix}
   0.042 \\[2pt] -0.0318 \\[2pt] -0.0268 \\[2pt] 1.72
 \end{bmatrix},
\]
matching the midpoint in \citet{DieboldLi2006}.  With this starting value, the optimiser converged to the local solution
\[
x^{*}
=\begin{bmatrix}
   0.0644 \\[2pt] -0.0291 \\[2pt] 1.28\times10^{-5} \\[2pt] 17.97
 \end{bmatrix}.
\]

\subsubsection*{1. Two-dimensional loss surface}

Figure \ref{fig:path-2d} visualises the loss landscape in the \((f_{0},\,f_{1})\) plane while the remaining parameters are held fixed at their previously‐optimised values \(\bigl(f_{2}=1.28\times10^{-5},\;\gamma=17.97\bigr)\).  
The background shading represents the normalised loss \(L(f_{0},f_{1})\): dark purple indicates low values, whereas green/yellow denotes high values.  

The red line shows the trajectory taken by the BFGS optimiser, which starts at the upper-right corner \([\,0.10,\,0.10\,]\) and, after a sharp curvature-driven turn, descends into the darker basin.  Convergence as shown is reached at point  
\[
x^{*} = [\,0.0644,\,-0.0291\,],
\]
where the stopping criteria are satisfied.  
The visible turns in the path shows the successive updates of the BFGS inverse-Hessian approximation as it adapts to the locally varying curvature of the non-convex surface.  
The contrast between flat plateaus and steep ridges on the loss surface explains why the optimiser is so sensitive to its starting point, as observed in earlier experiments.


\subsubsection*{2. Three-dimensional loss surface}
About the three-dimensional loss surface in Figure \ref{fig:path-3d}, we can get to know that about this cube $f_0\!\in[-0.06,0.10]$, $f_1\!\in[-0.10,0.10]$, $f_2\!\in[-0.10,0.10]$.  And darker points indicate lower loss values. The resulting surface reveals a narrow, almost linear valley running diagonally through the cloud: moving along this ridge leaves the loss nearly unchanged, while moving a small step laterally, the target value rises rapidly. This canyon explains the markedly different convergence behaviour observed in our analysis.

\subsubsection*{2. Three-dimensional loss surface}

Figure \ref{fig:path-3d} displays a three-dimensional slice of the loss surface, evaluated on the cube  
\[
f_{0}\in[-0.06,\,0.10],\qquad
f_{1}\in[-0.10,\,0.10],\qquad
f_{2}\in[-0.10,\,0.10],
\]
with \(\gamma\) fixed at its previously optimised value.

The cloud reveals a narrow, almost linear valley of dark-coloured points running diagonally through the cube.  
Moving \emph{along} this valley leaves the objective nearly unchanged, but even a small step in a transverse direction raises the loss sharply.  
This “canyon” landscape explain the reasons behind why the gradient-based algorithms sensitive to their initial guess and prone to large, kink-like turns (as seen in the two-dimensional path).  
In short, the 3-D view confirms why different starts can slide the optimiser along this flat ridge before it finally settles in a local minimum.


\begin{figure}[ht]
  \centering
  \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{2D.png}
    \caption{Optimisation path on the $f_0$–$f_1$ loss surface}
    \label{fig:path-2d}
  \end{minipage}
  \hspace{0.02\linewidth}
  \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{3D.png}
    \caption{Optimisation path on the 3-D loss surface}
    \label{fig:path-3d}
  \end{minipage}
\end{figure}

\subsection{Optimization Algorithms}

Let
\[
\boldsymbol{\theta}
=\bigl(f_{0},f_{1},f_{2},\gamma\bigr)^{\top}
\;\in\;\mathbb{R}^{3}\times[0,\infty),
\]
and denote by \(V_{j}(\boldsymbol{\theta})\) the model price of bond \(j=1,\dots,m\) (see \S\ref{subsec:bond_pricing}).  For observed bid–ask bounds \(\bigl[\mathrm{Bid}_j,\mathrm{Ask}_j\bigr]\), define the penalised loss
\[
L(\boldsymbol{\theta})
=\sum_{j=1}^{m}\ell_{j}(\boldsymbol{\theta}),
\quad
\ell_{j}(\boldsymbol{\theta})
=\Bigl[\tfrac{\max\bigl(0,\mathrm{Bid}_{j}-V_{j}(\boldsymbol{\theta})\bigr)}{\mathrm{Bid}_{j}}\Bigr]^{2}
+
\Bigl[\tfrac{\max\bigl(0,V_{j}(\boldsymbol{\theta})-\mathrm{Ask}_{j}\bigr)}{\mathrm{Ask}_{j}}\Bigr]^{2}.
\]
Let
\[
\boldsymbol{g}(\boldsymbol{\theta})
=\nabla L(\boldsymbol{\theta}),
\qquad
H(\boldsymbol{\theta})
=\nabla^{2} L(\boldsymbol{\theta}),
\]
with closed-form expressions given in \S\ref{sec:method}. The calibration proceeds through two deterministic iterative schemes,
both driven solely by the triplet \((L,\boldsymbol{g},H)\).

In this project, two approaches are taken: one manually coded from scratch, and the other using SciPy’s built-in optimizer. 

\subsubsection{Constraint on \(\gamma\)}

To enforce \(\gamma\ge0\) and avoid numerical overflow when penalty is active, two strategies are used:
\begin{enumerate}
  \item \emph{Reparameterisation:} set \(\gamma = e^{x}\) with \(x\in\mathbb{R}\).  This renders \(\gamma>0\) automatically and allows use of unconstrained methods (e.g.\ DFP or conjugate-gradient on \(x\)).
  \item \emph{Explicit constraint:} retain \(\gamma\ge0\) and employ constrained optimisers (e.g.\ gradient-projection or Zoutendijk’s method).
\end{enumerate}

Under both approaches, the zero-coupon discount factor is defined piecewise by continuity:
\[
B(\boldsymbol{\theta},T)
=
\begin{cases}
  e^{-T f_{0}}, & \gamma = 0,\\[6pt]
  \displaystyle
  \exp\!\Bigl\{-T f_{0}
    -\bigl[\gamma-\gamma e^{-T/\gamma}\bigr]\,f_{1}
    -\bigl[\gamma-(T+\gamma)e^{-T/\gamma}\bigr]\,f_{2}
  \Bigr\}, & \gamma>0.
\end{cases}
\]
The branch at \(\gamma=0\) follows by taking the limit \(\gamma\to0^{+}\), since for any fixed $t>0$, as \(\gamma\to0^+\):
            \[
            e^{-t/\gamma}=o(1)\to0,
            \qquad
            \frac{t}{\gamma}\,e^{-t/\gamma}=o(1)\to0.
            \]Consequently, the exponential term and its first derivative both vanish at the boundary $\gamma=0$, so the limit-extension of the loss is well-defined and continuous.



\subsubsection{Manual Implementation Methods}
To improve the speed of our manual optimization, SciPy’s L-BFGS-B is employed solely for line-search, while the core update formulas were coded from scratch. \newline

%--------------------
\textbf{1. DFP (Davidon–Fletcher–Powell) Method}
%--------------------
The DFP method is a quasi-Newton algorithm designed for unconstrained optimization. It assumes the objective function is continuously differentiable and builds an approximation to the inverse Hessian matrix using gradient information only. At each iteration, the search direction is computed as a product of the inverse Hessian approximation and the negative gradient. A line search is used to determine the step size. 

\textbf{2. Fletcher–Reeves Method (Conjugate Gradient) }
%----------------
The Conjugate Gradient (CG) method is suitable for large-scale unconstrained problems. Instead of following the steepest descent direction, CG computes mutually conjugate directions that allow faster convergence. The method avoids storing matrices explicitly, making it efficient in memory.

\textbf{3. Zoutendijk Method}
%----------------
Zoutendijk's method provides a theoretical framework for the convergence of constrained optimization algorithms that use descent directions and line search. The approach assumes the objective function is differentiable and that the step sizes satisfy conditions such as the Armijo rule. It ensures convergence to a stationary point by requiring that each descent direction forms a sufficiently small angle with the gradient.

%----------------
\textbf{4. Gradient Projection}
%----------------
The Gradient Projection Method is explicitly designed for constrained optimization where the feasible region is convex. At each iteration, it takes a step along the negative gradient and then projects the point back onto the feasible set to maintain feasibility. This method assumes that the objective function is continuously differentiable and the projection can be efficiently computed.


\subsubsection{SciPy Methods}

For unconstrained optimization, algorithms such as BFGS, Newton-CG, and CG are employed by implicitly enforcing the constraint $\gamma\geq0$. For constrained optimization, only L-BFGS-B algorithm is used as a benchmark. 

%----------
\textbf{1. BFGS and L-BFGS-B}

The BFGS (Broyden–Fletcher–Goldfarb–Shanno) method is a quasi-Newton algorithm that iteratively approximates the inverse Hessian matrix using gradient information. It is efficient for medium-scale unconstrained optimization and offers superlinear convergence for smooth problems. However, it requires storing a full \( n \times n \) matrix, which becomes impractical for high-dimensional problems. The L-BFGS-B (Limited-memory BFGS with Bounds) method addresses this limitation by storing only a few vectors that represent the approximation implicitly, reducing memory usage.

%----------
\textbf{2. Newton-CG }

The Newton-CG (also known as Trust-Region Newton-Conjugate Gradient) method is a second-order optimization algorithm designed for large-scale problems. It minimizes the objective function by solving the Newton system using an inner Conjugate Gradient solver rather than explicitly computing the inverse Hessian. The outer loop checks the gradient norm, and the inner loop solves for a descent direction up to a residual tolerance. 

%-----------
\textbf{3. Conjugate-Gradient (Polak-Ribiere)}
%-----------
The Polak–Ribiere variant of the Conjugate Gradient method is a first-order method suitable for smooth, unconstrained optimization problems. This method retains conjugacy across iterations, leading to faster convergence than steepest descent. To mitigate loss of conjugacy in non-quadratic or kinked regions, periodic restarts are applied. A tight gradient norm tolerance and relaxed Wolfe condition parameters are recommended for robust line search.

\subsection{Data}

The data used in this study are Australian Government Bonds obtained from the Australian Securities Exchange (ASX). These bonds are classified as Treasury Bonds, issued by the Australian Government, and are considered to be free of default risk. Compared to corporate bonds, government bonds are backed by the full faith and credit of the federal government, and thus serve as a benchmark for risk-free interest rates. The dataset includes bonds with various maturities and coupon rates, enabling the construction of a comprehensive term structure of interest rates. By analyzing bonds across different maturities, the shape and dynamics of the yield curve can be captured with higher accuracy. The implementation, as of 23 May 2025, is configured to automatically retrieve the latest available data from the ASX. This ensures that the project's yield curve estimation reflects the most up-to-date market conditions.

\section{Results \& Discussion}\label{sec:results}

\subsection{Gradient and Hessian function}

The gradient and Hessian of the loss function using three approaches: analytical derivation, numerical approximation, and automatic differentiation via the \texttt{Autograd} library. To evaluate efficiency, each method are compared in terms of accuracy and speed, taking the analytical method as the accuracy benchmark and the numerical method pre-compiled in C as the speed baseline. Among the three, \texttt{Autograd} achieved the best overall performance. This is likely due to its use of vectorized operations and internal computation graph, which minimizes redundant calculations. 

\subsection{Result of Optimization Method }

Table \ref{tab:summary} below summarizes the outcomes of the optimization algorithms applied to estimate the parameters of the Nelson-Siegel model. For each algorithm, the estimated parameters, the final loss value, and the success status of the optimization convergence are reported. In cases where the algorithm failed, the corresponding failure reasons are also noted. The fact that each algorithm estimated substantially different parameter values suggests the existence of multiple local minima in the loss function. This was further supported by experiments where varying the initial values, especially when they were far from the benchmark-led to different optima. This highlights the sensitivity of the model to initial parameter settings.
Notably, all implementations based on conjugate-gradient techniques failed to converge. This is likely a consequence of the loss function’s non-convex landscape, which can hinder line-search methods that rely solely on gradient information.

\label{sec:results-data}
\begin{sidewaystable}
\centering

\begin{tabular}{|l |l |l |l |l |l |l |l |l|}\hline       
 Algorithm & f0 & f1 & f2 & gamma & Loss Value & Itr& S/F& Message\\\hline
 DFP & 0.871& -0.829& -0.796& 108.526& 0.000013
& 24& S& Change of fun is within tolerence
\\\hline
  Fletcher-Reeves& -inf& -inf& inf& inf& 0.000000
& 3& S& Norm of gradient is within tolerence
\\\hline
  Gradient Projection & 0.053& -0.013& -0.032& 1.721& 2.090541
& 10& S& Change of x is within tolerence
\\\hline
  Zoutendijk & 0.042& -0.032& -0.027& 1.720& 1.362066
& 209& S& Change of x is within tolerence
\\\hline
 CG & 0.057& -0.019& -0.019& 4.285& 0.000032
& 159& S& Optimization terminated successfully.
\\\hline
  Newton-CG & 0.066& -0.028& -0.005& 14.234& 0.645016
& 300& F& Warning: Maximum number of iterations has been...
\\\hline
  L-BFGS-B & 0.071& -0.033& -0.007& 17.005& 0.000013
& 32& S& Convergence\\\hline
  BFGS & 0.066& -0.029& 0.000& 17.804& 6.521601
& 1& F& Desired error not necessarily achieved due to ...
\\ \hline

\end{tabular}
  \caption{Performance result of manual algorithm and SciPy solvers }
  \label{tab:summary}
\end{sidewaystable}



\clearpage
To gain a deeper understanding of how each algorithm performs during the optimization process, the loss path is visualized in two different ways. The Figure \ref{fig:iterationpath} illustrates the progression of the loss value over iterations, while the Figure \ref{fig:timepath} shows the loss trajectory with respect to computation time in seconds.
 As described earlier, some algorithms converged within just a few seconds, whereas others required over a minute to complete. Despite these differences in runtime, most algorithms—except for Gradient Projection and DFP—achieved similar final loss values.

\begin{figure}[ht]
  \centering
  \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{iteration.png}
    \caption{Loss path over iteration of all algorithm}
    \label{fig:iterationpath}
\end{minipage}
  \hspace{0.02\linewidth}
  \begin{minipage}[t]{0.48\linewidth}
    \centering

    \includegraphics[width=1\linewidth]{time.png}
    \caption{Loss path over time of all algorithm}
    \label{fig:timepath}
  \end{minipage}
\end{figure}
To identify the best-performing model, the resulting term structures of interest rates are visualised in Figure \ref{fig:interest rate} , which addresses the core objective of the project. In this comparison, algorithms that yielded highly unrealistic $\gamma$ values are excluded. For instance, although the DFP algorithm was very fast and achieved a low loss value, a transformation to $\gamma$ to impose a constraint during optimization was applied. However, the resulting $\gamma$ after optimization was excessively large, leading to an impractical value. In addition, algorithms which showed signs of overfitting are excluded as well, such as those requiring more than 300 iterations for convergence.
Among all the evaluated methods, L-BFGS-B delivers the most accurate fit. It not only achieved the lowest loss values but also produced term structures that closely aligned with the benchmark. However, its curves increased more steeply than what was observed in the benchmark, which is likely due to its relatively higher estimated $\gamma$ value, approximately around 17.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{interestrate.png}
    \caption{Estimated the term structure of interest rates}
    \label{fig:interest rate}
\end{figure}

\section{Conclusion}

The objective of this study was to refine the calibration of the Nelson–Siegel yield curve by embedding bid–ask information directly into the loss function. The loss function designed posed challenges due to non-convexity within the range of interest of our model parameters. The loss function is also found to be sensitive to starting point values, and while a fair attempt is carried out to explore a variety of starting points to mitigate this issue, a deeper analysis of initial-value dependence was not conducted. This is because the nature of the yield curve as an economic variable, tends to remain within a relatively stable range. As such, our focus was on identifying parameter estimates that closely match the benchmark curves. Two optimisation strategies were adopted to address the calibration problem. First, by incorporating a constraint on $\gamma$ directly within the objective function, enabling the use of unconstrained optimisation algorithms. Secondly, boundary conditions were introduced externally to support constrained optimisation. In this context, the the L-BFGS-B algorithm consistently yielded the lowest loss value. These algorithms also produced term structures that closely resemble the benchmark curve, although the long end tended to be steeper due to the large estimated values of $\gamma$.

\subsection{Limitations and future work} 

Due to the lack of availability of related research, validation methods on whether the obtained optimum for such loss function designs adequately reflects Australia's term structure of interest rates are limited. This study can also be expanded to explore different loss function designs that balances the tradeoffs between overfitting and retaining bid-ask spread information. Moreover, extending the analysis to bond markets in other countries would test the generality of the proposed calibration framework. A comparative study using multiple sovereign yield curves may also reveal which optimisation techniques and loss structures are most robust to differing liquidity profiles, market conventions, and data quality. Cross‐market validation would strengthen confidence in the method’s applicability beyond the Australian context.


\clearpage
\section*{Appendix A}

\subsection*{1. Gradient of the Loss}

Define
\[
L_j
 = \Bigl(\frac{\max\{0,V_j-\mathrm{Ask}_j\}}{A_j}\Bigr)^{2}
 + \Bigl(\frac{\max\{0,\mathrm{Bid}_j-V_j\}}{B_j}\Bigr)^{2}.
\]
Then
\begin{equation}
\frac{\partial L}{\partial\theta}
  =\sum_{j=1}^{m}\frac{\partial L_j}{\partial\theta},
  \qquad
  \theta\in\{f_0,f_1,f_2,\gamma\},
\end{equation}
with
\[
\frac{\partial L_j}{\partial\theta}=
\begin{cases}
 \dfrac{V_j-\mathrm{Ask}_j}{A_j^{2}}\,
 \dfrac{\partial V_j}{\partial\theta},
   & V_j>\mathrm{Ask}_j,\\[0.6em]
 0, & \mathrm{Bid}_j<V_j<\mathrm{Ask}_j,\\[0.6em]
 \dfrac{V_j-\mathrm{Bid}_j}{B_j^{2}}\,
 \dfrac{\partial V_j}{\partial\theta},
   & V_j<\mathrm{Bid}_j.
\end{cases}
\]

\paragraph{First derivatives of \(B(0,T)\).}
\begin{align}
\frac{\partial B}{\partial f_0} &= -T\,B(0,T),\\
\frac{\partial B}{\partial f_1} &= B(0,T)\,\bigl(-\gamma+\mathrm{e}^{-T/\gamma}\gamma\bigr),\\
\frac{\partial B}{\partial f_2} &= B(0,T)\,\bigl(-\gamma+\mathrm{e}^{-T/\gamma}(T+\gamma)\bigr),\\
\frac{\partial B}{\partial\gamma} &=
  B(0,T)\Bigl[
    -1-\mathrm{e}^{-T/\gamma}
    -\mathrm{e}^{-T/\gamma}\tfrac{T}{\gamma}\,f_1
    -\bigl(1+\mathrm{e}^{-T/\gamma}
           +\mathrm{e}^{-T/\gamma}\tfrac{T(T+\gamma)}{\gamma^{2}}\bigr)f_2
  \Bigr].
\end{align}

%-------------------------------------------------
\subsection*{2. Hessian of the Loss}

\begin{align}
\frac{\partial^{2} L}{\partial\theta_1\partial\theta_2}
  =
  \begin{cases}
  \sum_{j=1}^{m}  \dfrac{V_j-\mathrm{Ask}_j}{A_j^{2}}
      \dfrac{\partial^{2}V_j}{\partial\theta_1\partial\theta_2}
    +\dfrac{1}{A_j^{2}}
      \dfrac{\partial V_j}{\partial\theta_1}
      \dfrac{\partial V_j}{\partial\theta_2},
      & V_j>\mathrm{Ask}_j,\\[0.8em]
    0, & \mathrm{Bid}_j<V_j<\mathrm{Ask}_j,\\[0.8em]
   \sum_{j=1}^{m} \dfrac{V_j-\mathrm{Bid}_j}{B_j^{2}}
      \dfrac{\partial^{2}V_j}{\partial\theta_1\partial\theta_2}
    +\dfrac{1}{B_j^{2}}
      \dfrac{\partial V_j}{\partial\theta_1}
      \dfrac{\partial V_j}{\partial\theta_2},
      & V_j<\mathrm{Bid}_j,
  \end{cases}
\end{align}
where
\[
\frac{\partial^{2}V_j}{\partial\theta_1\partial\theta_2}
  =100\sum_{i=1}^{n_j} c_j
      \frac{\partial^{2}B(0,T^{(j)}_i)}
           {\partial\theta_1\partial\theta_2}
     +\frac{\partial^{2}B\!\bigl(0,T^{(j)}_{n_j}\bigr)}
           {\partial\theta_1\partial\theta_2}.
\]

%-------------------------------------------------
\subsection*{3. Second Derivatives of \(B(0,T)\)}

Let \(\theta_1,\theta_2\in\{f_0,f_1,f_2,\gamma\}\).
Every \(\partial^{2}B/\partial\theta_1\partial\theta_2\) term is listed below
(without algebraic simplification):

\begin{align}
\frac{\partial^{2}B}{\partial f_0^{2}}
  &= B(0,T)\,T^{2}, \\[0.2em]
\frac{\partial^{2}B}{\partial f_0\partial f_1}
  &= -B(0,T)\,T\bigl(-\gamma+\mathrm{e}^{-T/\gamma}\gamma\bigr), \\[0.2em]
\frac{\partial^{2}B}{\partial f_0\partial f_2}
  &= -B(0,T)\,T\bigl(-\gamma+\mathrm{e}^{-T/\gamma}(T+\gamma)\bigr), \\[0.2em]
\frac{\partial^{2}B}{\partial f_0\partial\gamma}
  &= -B(0,T)\Bigl[
        T
        +\bigl(1+\mathrm{e}^{-T/\gamma}
               +\mathrm{e}^{-T/\gamma}\tfrac{T}{\gamma}\bigr)f_1
        +\bigl(1+\mathrm{e}^{-T/\gamma}
               +\mathrm{e}^{-T/\gamma}\tfrac{T(T+\gamma)}{\gamma^{2}}\bigr)f_2
      \Bigr], \\[0.4em]
\frac{\partial^{2}B}{\partial f_1^{2}}
  &= B(0,T)\bigl(-\gamma+\mathrm{e}^{-T/\gamma}\gamma\bigr)^{2}, \\[0.2em]
\frac{\partial^{2}B}{\partial f_1\partial f_2}
  &= B(0,T)\bigl(-\gamma+\mathrm{e}^{-T/\gamma}\gamma\bigr)
               \bigl(-\gamma+\mathrm{e}^{-T/\gamma}(T+\gamma)\bigr), \\[0.2em]
\frac{\partial^{2}B}{\partial f_1\partial\gamma}
  &= B(0,T)\Bigl[
        -1+\mathrm{e}^{-T/\gamma}
        +\mathrm{e}^{-T/\gamma}\tfrac{T}{\gamma}
        -\bigl(1+\mathrm{e}^{-T/\gamma}
               +\mathrm{e}^{-T/\gamma}\tfrac{T}{\gamma}\bigr)f_1
        -\bigl(1+\mathrm{e}^{-T/\gamma}
               +\mathrm{e}^{-T/\gamma}\tfrac{T(T+\gamma)}{\gamma^{2}}\bigr)f_2
      \Bigr], \\[0.4em]
\frac{\partial^{2}B}{\partial f_2^{2}}
  &= B(0,T)\bigl(-\gamma+\mathrm{e}^{-T/\gamma}(T+\gamma)\bigr)^{2}, \\[0.2em]
\frac{\partial^{2}B}{\partial f_2\partial\gamma}
  &= B(0,T)\Bigl[
        -1+\mathrm{e}^{-T/\gamma}
        +\mathrm{e}^{-T/\gamma}\tfrac{T(T+\gamma)}{\gamma^{2}}
        +\bigl(-\gamma+\mathrm{e}^{-T/\gamma}(T+\gamma)\bigr) \\
  &\qquad\quad
        -\bigl(1+\mathrm{e}^{-T/\gamma}
               +\mathrm{e}^{-T/\gamma}\tfrac{T}{\gamma}\bigr)f_1
        -\bigl(1+\mathrm{e}^{-T/\gamma}
               +\mathrm{e}^{-T/\gamma}\tfrac{T(T+\gamma)}{\gamma^{2}}\bigr)f_2
      \Bigr], \\[0.4em]
\frac{\partial^{2}B}{\partial\gamma^{2}}
  &= B(0,T)\Bigl[
        \mathrm{e}^{-T/\gamma}\frac{T^{2}}{\gamma^{3}}\,f_1
      - 2\,\mathrm{e}^{-T/\gamma}\frac{T}{\gamma^{2}}
      - \mathrm{e}^{-T/\gamma}\frac{T^{2}(T+\gamma)}{\gamma^{4}} \\
  &\qquad\quad
      + 2\,\mathrm{e}^{-T/\gamma}\frac{T(T+\gamma)}{\gamma^{3}}\,f_2
      - \bigl(1+\mathrm{e}^{-T/\gamma}
              +\mathrm{e}^{-T/\gamma}\tfrac{T}{\gamma}\bigr)f_1
      - \bigl(1+\mathrm{e}^{-T/\gamma}
              +\mathrm{e}^{-T/\gamma}\tfrac{T(T+\gamma)}{\gamma^{2}}\bigr)f_2
      \Bigr].
\end{align}


\section*{Appendix B: Code Repository}
The source code used for all experiments and visualizations in this study is available at:\\
\url{https://github.com/Thaigithub/UTS_PO_Assignment}



\begin{thebibliography}{}

\bibitem[Nelson and Siegel(1987)]{NelsonSiegel1987}
Nelson, C.\,R.\ and Siegel, A.\,F. (1987).
Parsimonious modeling of yield curves.
\textit{Journal of Business}, 60(4), 473--489.

\bibitem[Gomes-Gon\c{c}alves \textit{et~al.}(2017)]{GomesGoncalvesGzylMayoral2017}
Gomes-Gon\c{c}alves, E., Gzyl, H.\ and Mayoral, S.\ (2017).
Calibration of short-rate term-structure models from bid–ask coupon-bond prices.
\textit{Physica A: Statistical Mechanics and its Applications}, 492, 1456--1472.

\bibitem[Lapshin and Sohatskaya(2020)]{LapshinSohatskaya2020}
Lapshin, V.\ and Sohatskaya, S.\ (2020).
Choosing the weighting coefficients for estimating the term structure from sovereign bonds.
\textit{International Review of Economics \& Finance}, 70, 635--648.

\bibitem[Diebold and Li(2006)]{DieboldLi2006}
Diebold, F.\,X.\ and Li, C. (2006).
Forecasting the term structure of government bond yields.
\textit{Journal of Econometrics}, 130(2), 337--364.

\end{thebibliography}

\end{document}