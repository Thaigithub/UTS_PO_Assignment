{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import autograd.numpy as np\n",
    "from autograd import grad, jacobian\n",
    "from scipy.optimize import OptimizeResult, line_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix definiteness\n",
    "\n",
    "- positive (semi-positive) definite : all eigenvalue positive (non-negative)\n",
    "- negative (semi-negative) definite : all eigenvalue negative (non-possible)\n",
    "- indefinite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def positive_definite(A):\n",
    "    \"\"\"\n",
    "    Check if the matrix A is positive definite.\n",
    "    \"\"\"\n",
    "    return np.all(la.eigvals(A) > 0)\n",
    "\n",
    "\n",
    "def negative_definite(A):\n",
    "    \"\"\"\n",
    "    Check if the matrix A is positive definite.\n",
    "    \"\"\"\n",
    "    return np.all(la.eigvals(A) < 0)\n",
    "\n",
    "\n",
    "def semi_positive_definite(A):\n",
    "    \"\"\"\n",
    "    Check if the matrix A is positive definite.\n",
    "    \"\"\"\n",
    "    return np.all(la.eigvals(A) >= 0)\n",
    "\n",
    "\n",
    "def semi_negative_definite(A):\n",
    "    \"\"\"\n",
    "    Check if the matrix A is positive definite.\n",
    "    \"\"\"\n",
    "    return np.all(la.eigvals(A) <= 0)\n",
    "\n",
    "\n",
    "A = np.array([[3, 1, 5], [1, 4, 2], [5, 2, 1]])\n",
    "\n",
    "\n",
    "positive_definite(A)  # Check if A is positive definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix symetry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def symetry(A: np.ndarray):\n",
    "    \"\"\"\n",
    "    Check if the matrix A is positive definite.\n",
    "    \"\"\"\n",
    "    if len(A.shape) != 2:\n",
    "        raise False\n",
    "    if A.shape[0] != A.shape[1]:\n",
    "        raise False\n",
    "    return np.allclose(A, A.T)\n",
    "\n",
    "\n",
    "A = np.array([[3, 1, 5], [1, 4, 2], [5, 2, 1]])\n",
    "symetry(A)  # Check if A is positive definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix inverse\n",
    "\n",
    "A square matrix is invertible if\n",
    "\n",
    "- determinant is not zero\n",
    "- SVD?\n",
    "- Eigenvalue?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-80.99999999999996)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-differentiation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " gradient =  [ 0. -3.] \n",
      " Hessian =  [[2. 1.]\n",
      " [1. 2.]] \n",
      " determinant =  2.9999999999999996\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return np.sum(x**2) + x[0] * x[1] - x[0] - 2 * x[1]\n",
    "\n",
    "\n",
    "grad_f = grad(f)\n",
    "hessen_f = jacobian(grad_f)\n",
    "x = np.array([1.0, -1.0])\n",
    "# print(\n",
    "#     \" gradient = \",\n",
    "#     grad_f(x),\n",
    "#     \"\\n Hessian = \",\n",
    "#     hessen_f(x),\n",
    "#     \"\\n determinant = \",\n",
    "#     la.det(hessen_f(x)),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steepest Descent Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(\n",
    "    fun, x0: np.ndarray, jac, ls=line_search, maxiter=100, amax=1000.0, tol=1.0e-8\n",
    "):\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    d_k = -jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation: \", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        alpha_k, _, _, _, _, success = ls(fun, jac, x_k, d_k, amax=amax)\n",
    "        if success is None:\n",
    "            print(\"Line search fail\")\n",
    "            break\n",
    "\n",
    "        print(\"alpha_k = \", alpha_k)\n",
    "        if abs(alpha_k * la.norm(jac(x_k))) < x_eps:\n",
    "            print(\"change of x is within tolerence\")\n",
    "            break\n",
    "\n",
    "        x_k1 = x_k + alpha_k * d_k\n",
    "        d_k1 = -jac(x_k1)\n",
    "\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fletcher-Reeves Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fletcher_reeves(fun, x0, jac, ls=line_search, maxiter=100, amax=1000., tol=1.e-8):\n",
    "    \"\"\"\n",
    "    Simple implementation of the Fletcherâ€“Reeves nonlinear conjugate-gradient method\n",
    "    for minimizing an unconstrained nonlinear function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fun : callable\n",
    "        The objective function to be minimized, fun(x) -> float.\n",
    "\n",
    "    x0 : 1D array\n",
    "        Initial guess.\n",
    "\n",
    "    jac : callable\n",
    "        The gradient of the objective, jac(x) -> array_like.\n",
    "\n",
    "    ls : callable, optional\n",
    "        A line-search routine (default: scipy.optimize.line_search).\n",
    "\n",
    "    maxiter : int, optional\n",
    "        Maximum number of iterations (default: 100).\n",
    "\n",
    "    amax : float, optional\n",
    "        Maximum step size in line search (default: 1000.).\n",
    "\n",
    "    tol : float, optional\n",
    "        Tolerance for stopping criteria (default: 1.e-8).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : scipy.optimize.OptimizeResult\n",
    "        The optimization result object with fields:\n",
    "        - x:     final solution\n",
    "        - fun:   function value at the solution\n",
    "        - nit:   number of iterations\n",
    "        - nfev:  number of function evaluations\n",
    "        - njev:  number of gradient evaluations\n",
    "        - success: True if a stopping criterion was met\n",
    "        - message: termination reason\n",
    "    \"\"\"\n",
    "\n",
    "    # Tolerances for various stopping criteria\n",
    "    x_eps = tol  # Tolerance on the step size\n",
    "    f_eps = tol  # Tolerance on the function change\n",
    "    g_eps = tol  # Tolerance on the gradient norm\n",
    "\n",
    "    # Initialize\n",
    "    x_k = np.array(x0, dtype=float).copy()\n",
    "    f_k = fun(x_k)\n",
    "    nfev = 1\n",
    "    g_k = jac(x_k)\n",
    "    njev = 1\n",
    "    nit = 0\n",
    "\n",
    "    # Prepare result container\n",
    "    res = OptimizeResult()\n",
    "\n",
    "    # Check initial gradient norm\n",
    "    if la.norm(g_k) < g_eps:\n",
    "        res.x = x_k\n",
    "        res.fun = f_k\n",
    "        res.nit = nit\n",
    "        res.nfev = nfev\n",
    "        res.njev = njev\n",
    "        res.success = True\n",
    "        res.status = 0\n",
    "        res.message = \"Initial gradient norm below tolerance\"\n",
    "        return res\n",
    "\n",
    "    # Initial direction: d0 = -g0\n",
    "    d_k = -g_k\n",
    "\n",
    "    while nit < maxiter:\n",
    "        nit += 1\n",
    "\n",
    "        # Line search along d_k\n",
    "        alpha_k, fc_k, gc_k, f_k_new, old_fval, old_old_fval = ls(\n",
    "            fun, jac, x_k, d_k, gfk=g_k, amax=amax\n",
    "        )\n",
    "        nfev += fc_k\n",
    "        njev += gc_k\n",
    "\n",
    "        # If line search fails\n",
    "        if alpha_k is None or f_k_new is None:\n",
    "            res.x = x_k\n",
    "            res.fun = f_k\n",
    "            res.nit = nit\n",
    "            res.nfev = nfev\n",
    "            res.njev = njev\n",
    "            res.success = False\n",
    "            res.status = 1\n",
    "            res.message = \"Line search failed\"\n",
    "            return res\n",
    "\n",
    "        # Candidate new point\n",
    "        x_k1 = x_k + alpha_k * d_k\n",
    "\n",
    "        # Check step size\n",
    "        step_norm = la.norm(alpha_k * d_k)\n",
    "        if step_norm < x_eps:\n",
    "            res.x = x_k1\n",
    "            res.fun = f_k_new\n",
    "            res.nit = nit\n",
    "            res.nfev = nfev\n",
    "            res.njev = njev\n",
    "            res.success = True\n",
    "            res.status = 0\n",
    "            res.message = \"Step size below tolerance\"\n",
    "            return res\n",
    "\n",
    "        # Check function change\n",
    "        if abs(f_k - f_k_new) < f_eps:\n",
    "            res.x = x_k1\n",
    "            res.fun = f_k_new\n",
    "            res.nit = nit\n",
    "            res.nfev = nfev\n",
    "            res.njev = njev\n",
    "            res.success = True\n",
    "            res.status = 0\n",
    "            res.message = \"Function change below tolerance\"\n",
    "            return res\n",
    "\n",
    "        # Compute new gradient\n",
    "        g_k1 = jac(x_k1)\n",
    "        njev += 1\n",
    "        g_k1_norm = la.norm(g_k1)\n",
    "        if g_k1_norm < g_eps:\n",
    "            # Converged by gradient norm\n",
    "            res.x = x_k1\n",
    "            res.fun = f_k_new\n",
    "            res.nit = nit\n",
    "            res.nfev = nfev\n",
    "            res.njev = njev\n",
    "            res.success = True\n",
    "            res.status = 0\n",
    "            res.message = \"Gradient norm below tolerance\"\n",
    "            return res\n",
    "\n",
    "        # Fletcher-Reeves beta\n",
    "        beta_k1 = np.dot(g_k1, g_k1) / np.dot(g_k, g_k)\n",
    "\n",
    "        # Update direction\n",
    "        d_k1 = -g_k1 + beta_k1 * d_k\n",
    "\n",
    "        # Prepare for next iteration\n",
    "        x_k = x_k1\n",
    "        f_k = f_k_new\n",
    "        g_k = g_k1\n",
    "        d_k = d_k1\n",
    "\n",
    "    # If we exit the loop, we hit max iterations\n",
    "    res.x = x_k\n",
    "    res.fun = f_k\n",
    "    res.nit = nit\n",
    "    res.nfev = nfev\n",
    "    res.njev = njev\n",
    "    res.success = False\n",
    "    res.status = 1\n",
    "    res.message = \"Maximum number of iterations reached\"\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TOFIX: Fletcher-Reeves Reset Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fletcher_reeves_reset(\n",
    "    fun,\n",
    "    x0: np.ndarray,\n",
    "    jac,\n",
    "    ls=line_search,\n",
    "    maxiter=100,\n",
    "    amax=1000.0,\n",
    "    tol=1.0e-8,\n",
    "    reset=100,\n",
    "):\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    d_k = -jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation:\", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        alpha_k, _, _, _, _, success = ls(fun, jac, x_k, d_k, amax=amax)\n",
    "        if success is None:\n",
    "            print(\"Line search fail\")\n",
    "            break\n",
    "\n",
    "        print(\"alpha_k = \", alpha_k)\n",
    "        if abs(alpha_k * la.norm(jac(x_k))) < x_eps:\n",
    "            print(\"change of x is within tolerence\")\n",
    "            break\n",
    "        x_k1 = x_k + alpha_k * d_k\n",
    "        d_k1 = None\n",
    "        if nit % reset == 0:\n",
    "            d_k1 = -jac(x_k1)\n",
    "        else:\n",
    "            d_k1 = -jac(x_k1) + (la.norm(jac(x_k1)) ** 2 / la.norm(jac(x_k)) ** 2) * d_k\n",
    "\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Newton Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(\n",
    "    fun,\n",
    "    x0: np.ndarray,\n",
    "    jac,\n",
    "    ls=line_search,\n",
    "    maxiter=100,\n",
    "    amax=1000.0,\n",
    "    tol=1.0e-8,\n",
    "):\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    hessian = jacobian(jac)\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    d_k = -la.inv(hessian(x_k)) @ jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation:\", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        x_k1 = x_k + d_k\n",
    "        d_k1 = -la.inv(hessian(x_k)) @ jac(x_k1)\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Modified Newton Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_modified(\n",
    "    fun,\n",
    "    x0: np.ndarray,\n",
    "    jac,\n",
    "    ls=line_search,\n",
    "    maxiter=100,\n",
    "    amax=1000.0,\n",
    "    tol=1.0e-8,\n",
    "):\n",
    "    x_eps = tol  # tolerence for convergence on delta x\n",
    "    f_eps = tol  # tolerence for convergence on delta f\n",
    "    g_eps = tol  # tolerence for convergence on norm of gradient\n",
    "    hessian = jacobian(jac)\n",
    "    x_k = x0.copy()\n",
    "    nit = 1\n",
    "    f_k = fun(x_k)\n",
    "    d_k = -la.inv(hessian(x_k)) @ jac(x_k)\n",
    "    if la.norm(jac(x_k)) < g_eps:\n",
    "        print(\"norm of gradient is within tolerence\")\n",
    "        return None\n",
    "    while True:\n",
    "        print(\"Interation:\", nit)\n",
    "        print(f\"x_{nit} = \", x_k)\n",
    "        print(f\"f_{nit} = \", f_k)\n",
    "        print(f\"g_{nit} = \", jac(x_k))\n",
    "        print(f\"d_{nit} = \", d_k)\n",
    "        alpha_k, _, _, _, _, success = ls(fun, jac, x_k, d_k, amax=amax)\n",
    "        if success is None:\n",
    "            print(\"Line search fail\")\n",
    "            break\n",
    "\n",
    "        print(\"alpha_k = \", alpha_k)\n",
    "        if abs(alpha_k * la.norm(d_k)) < x_eps:\n",
    "            print(\"change of x is within tolerence\")\n",
    "            break\n",
    "        x_k1 = x_k + alpha_k * d_k\n",
    "        d_k1 = -la.inv(hessian(x_k)) @ jac(x_k1)\n",
    "        if abs(f_k - fun(x_k1)) < f_eps:\n",
    "            print(\"change of fun is within tolerence\")\n",
    "            break\n",
    "        if la.norm(jac(x_k1)) < g_eps:\n",
    "            print(\"norm of gradient is within tolerence\")\n",
    "            break\n",
    "\n",
    "        if nit > maxiter:\n",
    "            print(\"Max iter reached\")\n",
    "            break\n",
    "        nit += 1\n",
    "        x_k = x_k1\n",
    "        f_k = fun(x_k1)\n",
    "        d_k = d_k1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Davidon-Fletcher-Powell Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfp(fun, x0, jac, H_k=None, ls=line_search, maxiter=100, amax=1000., tol=1.e-8, logs=True):\n",
    "    \"\"\"\n",
    "    Simple implementation of the DFP method for minimising an unconstrained \n",
    "    nonlinear function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    fun : callable\n",
    "        The scalar function to be minimized: fun(x) -> float\n",
    "\n",
    "    x0 : 1D array\n",
    "        Initial guess\n",
    "\n",
    "    jac : callable\n",
    "        The gradient function: jac(x) -> array_like\n",
    "\n",
    "    ls : callable, optional\n",
    "        A line-search routine, default is scipy.optimize.line_search\n",
    "\n",
    "    maxiter : int, optional\n",
    "        Maximum number of iterations (default: 100)\n",
    "\n",
    "    amax : float, optional\n",
    "        Maximum step size in the line search (default: 1000.)\n",
    "\n",
    "    tol : float, optional\n",
    "        Tolerance for stopping criteria (default: 1.e-8)\n",
    "\n",
    "    logs : boolean, optional\n",
    "        Option to print the parameters calculated for each step\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    res : scipy.optimize.OptimizeResult\n",
    "        The result of the optimization, with fields:\n",
    "        - x: the final point\n",
    "        - fun: the final function value\n",
    "        - nfev: number of function evaluations\n",
    "        - njev: number of gradient evaluations\n",
    "        - nit: number of iterations\n",
    "        - success: boolean indicating if convergence criteria met\n",
    "        - message: termination description\n",
    "    \"\"\"\n",
    "    # Tolerances for convergence\n",
    "    x_eps = tol  # Tolerance on step size\n",
    "    f_eps = tol  # Tolerance on function change\n",
    "    g_eps = tol  # Tolerance on gradient norm\n",
    "\n",
    "    x_k = x0.astype(float).copy()\n",
    "    f_k = fun(x_k)\n",
    "    nfev = 1\n",
    "    g_k = jac(x_k)\n",
    "    njev = 1\n",
    "    nit = 0\n",
    "\n",
    "    # Initialize H as the identity matrix\n",
    "    n = x_k.size\n",
    "    H_k = H_k if H_k is not None else np.eye(n)\n",
    "\n",
    "    # Prepare an OptimizeResult to store info\n",
    "    res = OptimizeResult()\n",
    "\n",
    "    # Check initial gradient norm\n",
    "    n_g_k = la.norm(g_k)\n",
    "    if n_g_k < g_eps:\n",
    "        res.x = x_k\n",
    "        res.fun = f_k\n",
    "        res.success = True\n",
    "        res.status = 0\n",
    "        res.message = \"Initial gradient norm is within tolerance\"\n",
    "        res.nfev = nfev\n",
    "        res.njev = njev\n",
    "        res.nit = nit\n",
    "        return res\n",
    "\n",
    "    while True:\n",
    "        nit += 1\n",
    "\n",
    "        # Compute search direction d_k = -H_k * g_k\n",
    "        d_k = -H_k.dot(g_k)\n",
    "        if logs:\n",
    "            print(f\"Direction ({nit}): {d_k}\")\n",
    "            print(f\"Grad ({nit}): {g_k}\")\n",
    "\n",
    "        # Line search to find alpha\n",
    "        alpha_k, fc_k, gc_k, f_k_new, old_fval, old_old_fval = ls(\n",
    "            fun, jac, x_k, d_k, gfk=g_k, amax=amax\n",
    "        )\n",
    "        nfev += fc_k\n",
    "        njev += gc_k\n",
    "        if logs:\n",
    "            print(f\"Alpha ({nit}): {alpha_k}\")\n",
    "\n",
    "        # If line search fails\n",
    "        if alpha_k is None or f_k_new is None:\n",
    "            res.x = x_k\n",
    "            res.fun = f_k\n",
    "            res.success = False\n",
    "            res.status = 1\n",
    "            res.message = \"Line search failed\"\n",
    "            res.nfev = nfev\n",
    "            res.njev = njev\n",
    "            res.nit = nit\n",
    "            return res\n",
    "\n",
    "        # Candidate new point\n",
    "        x_k1 = x_k + alpha_k * d_k\n",
    "        if logs:\n",
    "            print(f\"x_({nit}+1): {x_k1}\")\n",
    "\n",
    "        # Check step size\n",
    "        step_norm = la.norm(alpha_k * d_k)\n",
    "        if step_norm < x_eps:\n",
    "            res.x = x_k1\n",
    "            res.fun = f_k_new\n",
    "            res.success = True\n",
    "            res.status = 0\n",
    "            res.message = \"Step size below tolerance\"\n",
    "            res.nfev = nfev\n",
    "            res.njev = njev\n",
    "            res.nit = nit\n",
    "            return res\n",
    "\n",
    "        # Check function change\n",
    "        if abs(f_k - f_k_new) < f_eps:\n",
    "            res.x = x_k1\n",
    "            res.fun = f_k_new\n",
    "            res.success = True\n",
    "            res.status = 0\n",
    "            res.message = \"Function change below tolerance\"\n",
    "            res.nfev = nfev\n",
    "            res.njev = njev\n",
    "            res.nit = nit\n",
    "            return res\n",
    "\n",
    "        # Compute new gradient\n",
    "        g_k1 = jac(x_k1)\n",
    "        njev += 1\n",
    "        n_g_k1 = la.norm(g_k1)\n",
    "        if n_g_k1 < g_eps:\n",
    "            res.x = x_k1\n",
    "            res.fun = f_k_new\n",
    "            res.success = True\n",
    "            res.status = 0\n",
    "            res.message = \"Gradient norm below tolerance\"\n",
    "            res.nfev = nfev\n",
    "            res.njev = njev\n",
    "            res.nit = nit\n",
    "            return res\n",
    "\n",
    "        # Update H_k using the DFP formula\n",
    "        s_k = x_k1 - x_k       # = alpha_k * d_k\n",
    "        y_k = g_k1 - g_k\n",
    "        if logs:\n",
    "            print(f\"s_{nit} = {s_k}, y_{nit} = {y_k}\")\n",
    "\n",
    "        # Avoid division by zero in DFP updates\n",
    "        sTy = np.dot(s_k, y_k)\n",
    "        if abs(sTy) > 1e-14:\n",
    "            # First term: s_k s_k^T / (s_k^T y_k)\n",
    "            term1 = np.outer(s_k, s_k) / sTy\n",
    "        else:\n",
    "            term1 = np.zeros((n, n))\n",
    "\n",
    "        # For the second term: H_k y_k y_k^T H_k / (y_k^T H_k y_k)\n",
    "        Hy_k = H_k.dot(y_k)\n",
    "        yH_y = y_k.dot(Hy_k)\n",
    "        if abs(yH_y) > 1e-14:\n",
    "            term2 = np.outer(Hy_k, Hy_k) / yH_y\n",
    "        else:\n",
    "            term2 = np.zeros((n, n))\n",
    "\n",
    "        H_k = H_k + term1 - term2\n",
    "        if logs:\n",
    "            print(f\"H_{nit} = {H_k}\")\n",
    "\n",
    "        # Prepare for next iteration\n",
    "        x_k = x_k1\n",
    "        f_k = f_k_new\n",
    "        g_k = g_k1\n",
    "\n",
    "        # Check iteration limit\n",
    "        if nit >= maxiter:\n",
    "            res.x = x_k\n",
    "            res.fun = f_k\n",
    "            res.success = False\n",
    "            res.status = 1\n",
    "            res.message = \"Max iterations reached\"\n",
    "            res.nfev = nfev\n",
    "            res.njev = njev\n",
    "            res.nit = nit\n",
    "            return res\n",
    "\n",
    "    # Should not reach here, but just in case:\n",
    "    res.x = x_k\n",
    "    res.fun = f_k\n",
    "    res.success = False\n",
    "    res.status = 2\n",
    "    res.message = \"Exited loop unexpectedly\"\n",
    "    res.nfev = nfev\n",
    "    res.njev = njev\n",
    "    res.nit = nit\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
